{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding Visualization\n",
        "\n",
        "This notebook visualizes embeddings in 2D space using PCA and t-SNE to understand how the fine-tuned model clusters similar sentences together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import functions from the scripts directory\n",
        "from src.data.loaders import load_toy_dataset\n",
        "from src.models.embedding_pipeline import load_embeddinggemma_model, embed_texts\n",
        "from src.models.lora_setup import setup_lora_model\n",
        "from src.visualization.embedding_viz import (\n",
        "    reduce_to_2d_pca,\n",
        "    reduce_to_2d_tsne,\n",
        "    plot_paired_embeddings\n",
        ")\n",
        "from src.utils.paths import timestamped_path\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model and Data\n",
        "\n",
        "Load the fine-tuned model and compute embeddings for all sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model (use trained model if available, otherwise base with LoRA)\n",
        "tokenizer, base_model = load_embeddinggemma_model()\n",
        "model = setup_lora_model(base_model, r=16, lora_alpha=32, lora_dropout=0.1)\n",
        "\n",
        "# Load dataset\n",
        "train_data = load_toy_dataset()\n",
        "\n",
        "# Get all unique sentences\n",
        "all_sentences = [item[\"anchor\"] for item in train_data] + [item[\"positive\"] for item in train_data]\n",
        "\n",
        "# Compute embeddings\n",
        "embeddings = embed_texts(all_sentences, model, tokenizer)\n",
        "\n",
        "print(f\"Computed embeddings for {len(all_sentences)} sentences\")\n",
        "print(f\"Embedding shape: {embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PCA Visualization\n",
        "\n",
        "Use Principal Component Analysis to reduce 768-dimensional embeddings to 2D for visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce to 2D using PCA\n",
        "embeddings_2d_pca = reduce_to_2d_pca(embeddings)\n",
        "\n",
        "# Separate anchors and positives\n",
        "num_pairs = len(train_data)\n",
        "anchor_embeddings_2d = embeddings_2d_pca[:num_pairs]\n",
        "positive_embeddings_2d = embeddings_2d_pca[num_pairs:]\n",
        "\n",
        "# Plot paired embeddings\n",
        "plot_paired_embeddings(\n",
        "    anchor_embeddings_2d,\n",
        "    positive_embeddings_2d,\n",
        "    title=\"2D PCA of Sentence Embeddings (after fine-tuning)\",\n",
        "    save_path=str(timestamped_path(\"outputs/visualizations\", \"embedding_pca\", \"png\"))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## t-SNE Visualization\n",
        "\n",
        "Use t-SNE for a different perspective on the embedding space. t-SNE preserves local neighborhoods better than PCA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce to 2D using t-SNE\n",
        "# Note: t-SNE can be slow for larger datasets\n",
        "from src.visualization.embedding_viz import reduce_to_2d_tsne\n",
        "\n",
        "embeddings_2d_tsne = reduce_to_2d_tsne(embeddings, perplexity=min(30, len(embeddings) - 1))\n",
        "\n",
        "# Separate anchors and positives\n",
        "anchor_embeddings_2d_tsne = embeddings_2d_tsne[:num_pairs]\n",
        "positive_embeddings_2d_tsne = embeddings_2d_tsne[num_pairs:]\n",
        "\n",
        "# Plot paired embeddings\n",
        "plot_paired_embeddings(\n",
        "    anchor_embeddings_2d_tsne,\n",
        "    positive_embeddings_2d_tsne,\n",
        "    title=\"2D t-SNE of Sentence Embeddings (after fine-tuning)\",\n",
        "    save_path=str(timestamped_path(\"outputs/visualizations\", \"embedding_tsne\", \"png\"))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation\n",
        "\n",
        "After fine-tuning, we expect:\n",
        "- Each anchor (X) should be close to its corresponding positive (O) of the same color\n",
        "- Different pairs (different colors) should be separated in the embedding space\n",
        "- The model has learned to cluster semantically similar sentences together\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
