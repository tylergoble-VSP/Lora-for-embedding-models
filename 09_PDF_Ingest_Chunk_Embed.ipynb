{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Ingestion, Chunking, and Embedding\n",
        "\n",
        "This notebook demonstrates the complete pipeline for processing PDF documents:\n",
        "1. **PDF Ingestion**: Extract text from PDF files with robust error handling\n",
        "2. **Text Cleaning**: Normalize text, remove headers/footers, fix hyphenation\n",
        "3. **Sophisticated Chunking**: Detect sections, split into sentences, pack into token-aware chunks\n",
        "4. **Embedding**: Generate embeddings for all chunks using EmbeddingGemma\n",
        "5. **Index Building**: Create searchable indexes for efficient retrieval\n",
        "\n",
        "This workflow prepares PDF documents for question generation and fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary functions from src modules\n",
        "# All functions are defined in src/, not in notebooks\n",
        "from src.data.pdf_to_chunks_pipeline import pdf_to_chunks\n",
        "from src.pipelines.embed_chunks import embed_chunks, build_faiss_index, build_sklearn_index\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: PDF to Chunks\n",
        "\n",
        "The first step is to extract text from a PDF file and chunk it into manageable pieces. The `pdf_to_chunks()` function handles:\n",
        "- **PDF Extraction**: Uses pdfplumber (primary) with pypdf fallback\n",
        "- **Text Cleaning**: Normalizes whitespace, fixes hyphenation, removes headers/footers\n",
        "- **Section Detection**: Identifies headings and groups content into sections\n",
        "- **Token-Aware Chunking**: Packs sentences into chunks respecting token limits with overlap\n",
        "\n",
        "### Chunking Parameters\n",
        "\n",
        "- `max_tokens`: Maximum tokens per chunk (default: 512, matches EmbeddingGemma's typical max length)\n",
        "- `overlap_tokens`: Number of tokens to overlap between chunks (default: 64, prevents context loss at boundaries)\n",
        "- `min_tokens`: Minimum tokens per chunk (default: 128, filters out tiny fragments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
            "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
            "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
            "    (8.0) - (12.0)\n",
            "    \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Object of type int64 is not JSON serializable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m pdf_path = \u001b[33m\"\u001b[39m\u001b[33mdata/mobydick.pdf\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Update this path\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Process PDF into chunks\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# This function saves chunks to a timestamped parquet file automatically\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m chunks_df = \u001b[43mpdf_to_chunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Will use PDF filename as doc_id\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlap_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mChunk DataFrame columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(chunks_df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/spark-dev-workspace/Lora-for-embedding-models/src/data/pdf_to_chunks_pipeline.py:134\u001b[39m, in \u001b[36mpdf_to_chunks\u001b[39m\u001b[34m(pdf_path, doc_id, tokenizer, max_tokens, overlap_tokens, min_tokens, min_heading_score)\u001b[39m\n\u001b[32m    132\u001b[39m sidecar_path = timestamped_path(\u001b[33m\"\u001b[39m\u001b[33mdata/processed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpdf_chunks_metadata\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(sidecar_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43msidecar_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    137\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved metadata to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msidecar_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: Object of type int64 is not JSON serializable"
          ]
        }
      ],
      "source": [
        "# Specify the path to your PDF file\n",
        "# Replace with your actual PDF path\n",
        "pdf_path = \"data/mobydick.pdf\"  # Update this path\n",
        "\n",
        "# Process PDF into chunks\n",
        "# This function saves chunks to a timestamped parquet file automatically\n",
        "chunks_df = pdf_to_chunks(\n",
        "    pdf_path=pdf_path,\n",
        "    doc_id=None,  # Will use PDF filename as doc_id\n",
        "    max_tokens=512,\n",
        "    overlap_tokens=64,\n",
        "    min_tokens=128\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(chunks_df)} chunks\")\n",
        "print(f\"\\nChunk DataFrame columns: {list(chunks_df.columns)}\")\n",
        "print(f\"\\nFirst few chunks:\")\n",
        "chunks_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Explore Chunk Statistics\n",
        "\n",
        "Let's examine the characteristics of our chunks to understand the document structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display chunk statistics\n",
        "print(\"Chunk Statistics:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total chunks: {len(chunks_df)}\")\n",
        "print(f\"Average tokens per chunk: {chunks_df['token_count'].mean():.1f}\")\n",
        "print(f\"Min tokens: {chunks_df['token_count'].min()}\")\n",
        "print(f\"Median tokens: {chunks_df['token_count'].median():.1f}\")\n",
        "print(f\"Max tokens: {chunks_df['token_count'].max()}\")\n",
        "print(f\"\\nUnique sections: {chunks_df['section_id'].nunique()}\")\n",
        "print(f\"Pages covered: {chunks_df['page_start'].min()} to {chunks_df['page_end'].max()}\")\n",
        "\n",
        "# Show distribution of chunk sizes\n",
        "print(\"\\nToken count distribution:\")\n",
        "print(chunks_df['token_count'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: View Example Chunks\n",
        "\n",
        "Let's examine a few example chunks to understand their structure and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display a few example chunks with their metadata\n",
        "for idx in range(min(3, len(chunks_df))):\n",
        "    chunk = chunks_df.iloc[idx]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Chunk {idx + 1}: {chunk['chunk_id']}\")\n",
        "    print(f\"Section: {chunk['section_title']}\")\n",
        "    print(f\"Pages: {chunk['page_start']}-{chunk['page_end']}\")\n",
        "    print(f\"Tokens: {chunk['token_count']}, Characters: {chunk['char_count']}\")\n",
        "    print(f\"\\nText preview (first 200 chars):\")\n",
        "    print(chunk['text'][:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Embed Chunks\n",
        "\n",
        "Now we'll generate embeddings for all chunks using EmbeddingGemma. This step:\n",
        "- Processes chunks in batches for efficiency\n",
        "- Uses the existing `embed_texts()` function with max_length=512\n",
        "- Saves embeddings to timestamped .npy file\n",
        "- Saves metadata to timestamped parquet file\n",
        "\n",
        "### Embedding Parameters\n",
        "\n",
        "- `batch_size`: Number of chunks to process at once (default: 64, adjust based on GPU memory)\n",
        "- `max_length`: Maximum sequence length (should match chunk max_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for all chunks\n",
        "# This function saves embeddings automatically to timestamped files\n",
        "embeddings_array, embeddings_metadata_df = embed_chunks(\n",
        "    chunks_df,\n",
        "    batch_size=64,  # Adjust based on GPU memory\n",
        "    max_length=512   # Should match chunk max_tokens\n",
        ")\n",
        "\n",
        "print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
        "print(f\"Embedding dimension: {embeddings_array.shape[1]}\")\n",
        "print(f\"\\nEmbeddings metadata:\")\n",
        "print(embeddings_metadata_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build Search Index (Optional)\n",
        "\n",
        "For efficient similarity search, we can build a FAISS or scikit-learn index. This allows us to quickly find the most similar chunks to a query.\n",
        "\n",
        "### Index Types\n",
        "\n",
        "- **FAISS (Flat)**: Exact nearest neighbor search, fastest for small-medium datasets\n",
        "- **FAISS (IVF)**: Approximate search with clustering, faster for large datasets\n",
        "- **scikit-learn NearestNeighbors**: Lightweight alternative, no extra dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to build FAISS index (requires faiss-cpu or faiss-gpu)\n",
        "faiss_index = build_faiss_index(embeddings_array, index_type=\"flat\")\n",
        "\n",
        "# If FAISS is not available, try scikit-learn\n",
        "if faiss_index is None:\n",
        "    print(\"FAISS not available, trying scikit-learn...\")\n",
        "    sklearn_index = build_sklearn_index(embeddings_array, n_neighbors=10)\n",
        "    if sklearn_index is not None:\n",
        "        print(\"Built scikit-learn index successfully\")\n",
        "    else:\n",
        "        print(\"Neither FAISS nor scikit-learn available. Install with: pip install faiss-cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Nearest Neighbor Search Example\n",
        "\n",
        "Let's demonstrate how to use the index to find similar chunks. This is useful for:\n",
        "- **Retrieval**: Finding relevant chunks for a query\n",
        "- **Hard Negative Mining**: Finding similar but non-matching chunks for contrastive learning\n",
        "- **Exploration**: Understanding document structure and relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Find chunks similar to a query\n",
        "from src.models.embedding_pipeline import load_embeddinggemma_model, embed_texts\n",
        "\n",
        "# Load model for query embedding\n",
        "tokenizer, model = load_embeddinggemma_model()\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "# Example query\n",
        "query_text = \"What is the main topic of this document?\"\n",
        "\n",
        "# Embed the query\n",
        "query_embedding = embed_texts(\n",
        "    query_text,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device=device,\n",
        "    max_length=512\n",
        ").numpy()\n",
        "\n",
        "# Find nearest neighbors (using simple cosine similarity if no index)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute similarities\n",
        "similarities = cosine_similarity(query_embedding, embeddings_array)[0]\n",
        "\n",
        "# Get top 5 most similar chunks\n",
        "top_k = 5\n",
        "top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "print(f\"Top {top_k} most similar chunks to query: '{query_text}'\")\n",
        "print(\"=\" * 60)\n",
        "for i, idx in enumerate(top_indices):\n",
        "    chunk = chunks_df.iloc[idx]\n",
        "    print(f\"\\nRank {i+1} (similarity: {similarities[idx]:.3f}):\")\n",
        "    print(f\"  Section: {chunk['section_title']}\")\n",
        "    print(f\"  Text preview: {chunk['text'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. ✅ PDF ingestion and text extraction\n",
        "2. ✅ Text cleaning and normalization\n",
        "3. ✅ Sophisticated chunking with section detection\n",
        "4. ✅ Embedding generation for all chunks\n",
        "5. ✅ Index building for efficient retrieval\n",
        "6. ✅ Nearest neighbor search example\n",
        "\n",
        "**Next Steps:**\n",
        "- Proceed to notebook `10_Generate_Questions_Build_Dataset.ipynb` to generate questions and build the training dataset\n",
        "- The chunks and embeddings are saved to timestamped files in `data/processed/` and `outputs/embeddings/`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
