{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 00: DGX Spark CUDA Setup\n",
        "\n",
        "**This notebook is specifically designed for NVIDIA DGX Spark systems.**\n",
        "\n",
        "This notebook should be run **BEFORE** `01_Setup_Environment.ipynb` if you're on a DGX Spark system. It ensures PyTorch is installed with proper CUDA support for the DGX Spark's GPU.\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "1. **Detects DGX Spark System**: Identifies ARM64 architecture and NVIDIA GB10 GPU\n",
        "2. **Checks Current PyTorch**: Determines if CPU-only or CUDA-enabled version is installed\n",
        "3. **Installs CUDA-Enabled PyTorch**: Installs PyTorch with CUDA 13.0 support (cu130)\n",
        "4. **Verifies GPU Access**: Confirms PyTorch can see and use the GPU\n",
        "5. **Provides System Information**: Shows GPU details, CUDA version, and PyTorch configuration\n",
        "\n",
        "## About DGX Spark\n",
        "\n",
        "The **NVIDIA DGX Spark** is a compact AI workstation featuring:\n",
        "- **Grace CPU**: ARM64-based processor (aarch64 architecture)\n",
        "- **NVIDIA GB10 GPU**: Blackwell architecture with 128 GB unified memory\n",
        "- **Compute Capability**: SM_121 (12.1) - newer than older PyTorch builds recognized\n",
        "- **CUDA Version**: 13.0\n",
        "\n",
        "**Why This Notebook is Needed:**\n",
        "- Default `pip install torch` installs CPU-only version on ARM64\n",
        "- DGX Spark requires CUDA 13.0 enabled PyTorch (cu130 index)\n",
        "- PyTorch 2.9+ has official support for CUDA 13.0 and DGX Spark\n",
        "- Without proper setup, GPU acceleration won't work\n",
        "\n",
        "**After This Notebook:**\n",
        "- Proceed to `01_Setup_Environment.ipynb` for full environment setup\n",
        "- Your PyTorch will have GPU support enabled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DGX SPARK CUDA SETUP\n",
            "======================================================================\n",
            "\n",
            "Project root: /home/goble54/spark-dev-workspace/Lora-for-embedding-models\n",
            "Python version: 3.12.3\n",
            "Python executable: /home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/bin/python\n",
            "Platform: Linux-6.11.0-1016-nvidia-aarch64-with-glibc2.39\n",
            "Architecture: aarch64\n"
          ]
        }
      ],
      "source": [
        "# Setup: Import system utilities and check architecture\n",
        "# We need to detect if we're on a DGX Spark system and check current PyTorch installation\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import platform\n",
        "from pathlib import Path\n",
        "\n",
        "# Get project root (for path management)\n",
        "# Notebooks are in the repository root directory\n",
        "# This handles both cases: if notebook is in root or in a subdirectory\n",
        "current_dir = Path(os.getcwd())\n",
        "if current_dir.name == \"notebooks\":\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    project_root = current_dir\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DGX SPARK CUDA SETUP\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nProject root: {project_root}\")\n",
        "print(f\"Python version: {sys.version.split()[0]}\")\n",
        "print(f\"Python executable: {sys.executable}\")\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "print(f\"Architecture: {platform.machine()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Detect DGX Spark System\n",
        "\n",
        "We'll check if this is a DGX Spark system by examining:\n",
        "- Architecture (should be ARM64/aarch64)\n",
        "- GPU presence (NVIDIA GB10)\n",
        "- CUDA version (should be 13.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Architecture: aarch64\n",
            "âœ“ ARM64 architecture detected (DGX Spark uses ARM64)\n",
            "\n",
            "âœ“ NVIDIA GPU Detected:\n",
            "  GPU Name: NVIDIA GB10\n",
            "  Driver Version: 580.95.05\n",
            "  Compute Capability: 12.1\n",
            "  âœ“ This appears to be a DGX Spark (GB10 GPU)\n",
            "\n",
            "âœ“ System CUDA Version: 13.0\n"
          ]
        }
      ],
      "source": [
        "# Check system architecture\n",
        "# DGX Spark uses ARM64 (aarch64) architecture, not x86_64\n",
        "# This is important because PyTorch builds differ by architecture\n",
        "architecture = platform.machine()\n",
        "is_arm64 = architecture == \"aarch64\" or architecture == \"arm64\"\n",
        "\n",
        "print(f\"System Architecture: {architecture}\")\n",
        "if is_arm64:\n",
        "    print(\"âœ“ ARM64 architecture detected (DGX Spark uses ARM64)\")\n",
        "else:\n",
        "    print(f\"âš  Expected ARM64 (aarch64) for DGX Spark, got {architecture}\")\n",
        "    print(\"  This notebook is designed for DGX Spark, but may work on other systems\")\n",
        "\n",
        "# Check for NVIDIA GPU using nvidia-smi\n",
        "# This confirms we have an NVIDIA GPU (DGX Spark has GB10)\n",
        "gpu_detected = False\n",
        "gpu_name = None\n",
        "cuda_version = None\n",
        "\n",
        "try:\n",
        "    # Run nvidia-smi to get GPU information\n",
        "    # nvidia-smi is the standard NVIDIA tool for GPU information\n",
        "    # Why nvidia-smi? Works on all NVIDIA systems, shows GPU name and CUDA version\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=name,driver_version,compute_cap', '--format=csv,noheader'],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=10\n",
        "    )\n",
        "    \n",
        "    if result.returncode == 0 and result.stdout.strip():\n",
        "        # Parse GPU information\n",
        "        # Format: \"GPU Name, Driver Version, Compute Capability\"\n",
        "        gpu_info = result.stdout.strip().split('\\n')[0]  # Get first GPU\n",
        "        parts = [p.strip() for p in gpu_info.split(',')]\n",
        "        if len(parts) >= 3:\n",
        "            gpu_name = parts[0]\n",
        "            driver_version = parts[1]\n",
        "            compute_cap = parts[2]\n",
        "            gpu_detected = True\n",
        "            \n",
        "            print(f\"\\nâœ“ NVIDIA GPU Detected:\")\n",
        "            print(f\"  GPU Name: {gpu_name}\")\n",
        "            print(f\"  Driver Version: {driver_version}\")\n",
        "            print(f\"  Compute Capability: {compute_cap}\")\n",
        "            \n",
        "            # Check if it's GB10 (DGX Spark's GPU)\n",
        "            if \"GB10\" in gpu_name.upper():\n",
        "                print(\"  âœ“ This appears to be a DGX Spark (GB10 GPU)\")\n",
        "            else:\n",
        "                print(f\"  âš  GPU is not GB10 (expected for DGX Spark)\")\n",
        "except (FileNotFoundError, subprocess.TimeoutExpired) as e:\n",
        "    print(f\"\\nâš  nvidia-smi not available or timed out\")\n",
        "    print(\"  This might not be a DGX Spark system, or NVIDIA drivers not installed\")\n",
        "\n",
        "# Get CUDA version from nvidia-smi\n",
        "# This shows the CUDA version supported by the driver\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
        "    if result.returncode == 0:\n",
        "        for line in result.stdout.split('\\n'):\n",
        "            if 'CUDA Version:' in line:\n",
        "                # Extract CUDA version (e.g., \"13.0\" from \"CUDA Version: 13.0\")\n",
        "                parts = line.split('CUDA Version:')\n",
        "                if len(parts) > 1:\n",
        "                    cuda_version = parts[1].strip().split()[0]\n",
        "                    print(f\"\\nâœ“ System CUDA Version: {cuda_version}\")\n",
        "                    break\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if not gpu_detected:\n",
        "    print(\"\\nâš  No NVIDIA GPU detected!\")\n",
        "    print(\"  This notebook is designed for DGX Spark systems with NVIDIA GPUs\")\n",
        "    print(\"  You can still proceed, but GPU acceleration won't be available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Check Current PyTorch Installation\n",
        "\n",
        "We need to check if PyTorch is already installed and whether it has CUDA support. On DGX Spark, the default `pip install torch` often installs a CPU-only version, which won't use the GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current PyTorch Installation:\n",
            "  Installed: Yes\n",
            "  Version: 2.9.1+cpu\n",
            "  CUDA Support: No (CPU-only)\n",
            "\n",
            "âš  PyTorch is CPU-only - GPU acceleration not available\n",
            "  This is the common issue on DGX Spark!\n",
            "  We'll fix this in the next step.\n",
            "\n",
            "======================================================================\n",
            "ACTION NEEDED: PyTorch CPU-only version detected\n",
            "======================================================================\n",
            "We need to:\n",
            "  1. Uninstall CPU-only PyTorch\n",
            "  2. Install PyTorch with CUDA 13.0 support (cu130)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Try to import PyTorch and check its configuration\n",
        "# This tells us if PyTorch is installed and whether it has CUDA support\n",
        "pytorch_installed = False\n",
        "pytorch_has_cuda = False\n",
        "pytorch_version = None\n",
        "pytorch_cuda_version = None\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    pytorch_installed = True\n",
        "    pytorch_version = torch.__version__\n",
        "    pytorch_has_cuda = torch.version.cuda is not None\n",
        "    pytorch_cuda_version = torch.version.cuda if pytorch_has_cuda else None\n",
        "    \n",
        "    print(\"Current PyTorch Installation:\")\n",
        "    print(f\"  Installed: Yes\")\n",
        "    print(f\"  Version: {pytorch_version}\")\n",
        "    print(f\"  CUDA Support: {'Yes' if pytorch_has_cuda else 'No (CPU-only)'}\")\n",
        "    if pytorch_has_cuda:\n",
        "        print(f\"  CUDA Version: {pytorch_cuda_version}\")\n",
        "    \n",
        "    # Check if GPU is accessible\n",
        "    if pytorch_has_cuda:\n",
        "        cuda_available = torch.cuda.is_available()\n",
        "        device_count = torch.cuda.device_count() if cuda_available else 0\n",
        "        \n",
        "        print(f\"\\nGPU Access:\")\n",
        "        print(f\"  CUDA Available: {cuda_available}\")\n",
        "        if cuda_available:\n",
        "            print(f\"  GPU Count: {device_count}\")\n",
        "            if device_count > 0:\n",
        "                gpu_name_pytorch = torch.cuda.get_device_name(0)\n",
        "                print(f\"  GPU Name: {gpu_name_pytorch}\")\n",
        "                print(\"  âœ“ PyTorch can access the GPU!\")\n",
        "            else:\n",
        "                print(\"  âš  No GPUs detected by PyTorch\")\n",
        "        else:\n",
        "            print(\"  âš  CUDA not available (driver or runtime issue)\")\n",
        "    else:\n",
        "        print(\"\\nâš  PyTorch is CPU-only - GPU acceleration not available\")\n",
        "        print(\"  This is the common issue on DGX Spark!\")\n",
        "        print(\"  We'll fix this in the next step.\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"Current PyTorch Installation:\")\n",
        "    print(\"  Installed: No\")\n",
        "    print(\"  PyTorch is not installed yet\")\n",
        "    print(\"  We'll install it with CUDA support in the next step\")\n",
        "except Exception as e:\n",
        "    print(f\"Error checking PyTorch: {e}\")\n",
        "\n",
        "# Determine if we need to reinstall\n",
        "needs_installation = not pytorch_installed\n",
        "needs_reinstall = pytorch_installed and not pytorch_has_cuda\n",
        "\n",
        "if needs_reinstall:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ACTION NEEDED: PyTorch CPU-only version detected\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"We need to:\")\n",
        "    print(\"  1. Uninstall CPU-only PyTorch\")\n",
        "    print(\"  2. Install PyTorch with CUDA 13.0 support (cu130)\")\n",
        "    print(\"=\" * 70)\n",
        "elif needs_installation:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ACTION NEEDED: PyTorch not installed\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"We'll install PyTorch with CUDA 13.0 support\")\n",
        "    print(\"=\" * 70)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âœ“ PyTorch with CUDA is already installed!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"You can skip the installation step and proceed to notebook 01\")\n",
        "    print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install PyTorch with CUDA 13.0 Support\n",
        "\n",
        "For DGX Spark, we need to install PyTorch from the CUDA 13.0 wheel index (`cu130`). This provides:\n",
        "- CUDA 13.0 support (matches your system CUDA version)\n",
        "- ARM64 (aarch64) builds for DGX Spark's Grace CPU\n",
        "- Support for GB10 GPU (compute capability 12.1)\n",
        "\n",
        "**Why cu130 index?**\n",
        "- PyTorch 2.9+ has official CUDA 13.0 support\n",
        "- Default pip install gives CPU-only on ARM64\n",
        "- cu130 index provides GPU-enabled builds for ARM64\n",
        "- Matches DGX Spark's CUDA 13.0 system\n",
        "\n",
        "**Installation Process:**\n",
        "1. Uninstall existing CPU-only PyTorch (if present)\n",
        "2. Install from cu130 index: `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130`\n",
        "3. Verify installation\n",
        "\n",
        "**Important:** After installation, you MUST restart the kernel for changes to take effect!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using virtual environment pip: /home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/bin/pip\n",
            "\n",
            "======================================================================\n",
            "PYTORCH INSTALLATION COMMAND\n",
            "======================================================================\n",
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/bin/pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "INSTALLING PyTorch with CUDA 13.0 support...\n",
            "======================================================================\n",
            "âš  This will take several minutes (downloading ~2GB of packages)\n",
            "âš  After installation, RESTART THE KERNEL (Kernel â†’ Restart Kernel)\n",
            "======================================================================\n",
            "\n",
            "Step 1: Uninstalling CPU-only PyTorch...\n",
            "âœ“ CPU-only PyTorch uninstalled\n",
            "\n",
            "Step 2: Installing PyTorch with CUDA 13.0 support...\n",
            "This downloads and installs:\n",
            "  - torch (PyTorch core with CUDA 13.0)\n",
            "  - torchvision (computer vision utilities)\n",
            "  - torchaudio (audio processing utilities)\n",
            "  - NVIDIA CUDA libraries (cudnn, cublas, etc.)\n",
            "\n",
            "Downloading... (this may take 5-10 minutes)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu130\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu130/torch-2.9.1%2Bcu130-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (30 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu130/torchvision-0.24.1-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (5.9 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu130/torchaudio-2.9.1-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch) (2026.1.0)\n",
            "Collecting nvidia-cuda-nvrtc==13.0.48 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc/nvidia_cuda_nvrtc-13.0.48-py3-none-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (43.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.0/43.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime==13.0.48 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime/nvidia_cuda_runtime-13.0.48-py3-none-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti==13.0.48 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-cupti/nvidia_cuda_cupti-13.0.48-py3-none-manylinux_2_25_aarch64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu13==9.13.0.50 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu13/nvidia_cudnn_cu13-9.13.0.50-py3-none-manylinux_2_27_aarch64.whl (412.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m412.3/412.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m  \u001b[33m0:00:16\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas==13.0.0.19 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cublas/nvidia_cublas-13.0.0.19-py3-none-manylinux_2_27_aarch64.whl (539.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m539.0/539.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m  \u001b[33m0:00:22\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft==12.0.0.15 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cufft/nvidia_cufft-12.0.0.15-py3-none-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (214.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m214.1/214.1 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand==10.4.0.35 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-curand/nvidia_curand-10.4.0.35-py3-none-manylinux_2_27_aarch64.whl (62.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver==12.0.3.29 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cusolver/nvidia_cusolver-12.0.3.29-py3-none-manylinux_2_27_aarch64.whl (193.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse==12.6.2.49 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cusparse/nvidia_cusparse-12.6.2.49-py3-none-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (155.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.9/155.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparselt-cu13==0.8.0 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cusparselt-cu13/nvidia_cusparselt_cu13-0.8.0-py3-none-manylinux2014_aarch64.whl (220.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.8/220.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m  \u001b[33m0:00:14\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu13==2.27.7 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu13/nvidia_nccl_cu13-2.27.7-py3-none-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (194.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.0/194.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvshmem-cu13==3.3.24 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nvshmem-cu13/nvidia_nvshmem_cu13-3.3.24-py3-none-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (60.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.2/60.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx==13.0.39 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nvtx/nvidia_nvtx-13.0.39-py3-none-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (148 kB)\n",
            "Collecting nvidia-nvjitlink==13.0.39 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nvjitlink/nvidia_nvjitlink-13.0.39-py3-none-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (38.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufile==1.15.0.42 (from torch)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cufile/nvidia_cufile-1.15.0.42-py3-none-manylinux_2_27_aarch64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.5.1 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/triton-3.5.1-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (12.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
            "Using cached https://download.pytorch.org/whl/cu130/torch-2.9.1%2Bcu130-cp312-cp312-manylinux_2_28_aarch64.whl (512.4 MB)\n",
            "Using cached https://download.pytorch.org/whl/triton-3.5.1-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (159.9 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu130/torchvision-0.24.1-cp312-cp312-manylinux_2_28_aarch64.whl (7.8 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu130/torchaudio-2.9.1-cp312-cp312-manylinux_2_28_aarch64.whl (1.9 MB)\n",
            "Installing collected packages: nvidia-cusparselt-cu13, triton, nvidia-nvtx, nvidia-nvshmem-cu13, nvidia-nvjitlink, nvidia-nccl-cu13, nvidia-curand, nvidia-cufile, nvidia-cuda-runtime, nvidia-cuda-nvrtc, nvidia-cuda-cupti, nvidia-cublas, nvidia-cusparse, nvidia-cufft, nvidia-cudnn-cu13, nvidia-cusolver, torch, torchvision, torchaudio\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19/19\u001b[0m [torchaudio]9\u001b[0m [torchaudio]]ver]3]3]\n",
            "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-13.0.0.19 nvidia-cuda-cupti-13.0.48 nvidia-cuda-nvrtc-13.0.48 nvidia-cuda-runtime-13.0.48 nvidia-cudnn-cu13-9.13.0.50 nvidia-cufft-12.0.0.15 nvidia-cufile-1.15.0.42 nvidia-curand-10.4.0.35 nvidia-cusolver-12.0.3.29 nvidia-cusparse-12.6.2.49 nvidia-cusparselt-cu13-0.8.0 nvidia-nccl-cu13-2.27.7 nvidia-nvjitlink-13.0.39 nvidia-nvshmem-cu13-3.3.24 nvidia-nvtx-13.0.39 torch-2.9.1+cu130 torchaudio-2.9.1 torchvision-0.24.1 triton-3.5.1\n",
            "\n",
            "======================================================================\n",
            "âœ“ PyTorch with CUDA 13.0 installed successfully!\n",
            "======================================================================\n",
            "\n",
            "âš  CRITICAL NEXT STEPS:\n",
            "  1. Go to: Kernel â†’ Restart Kernel (or Kernel â†’ Restart & Clear Output)\n",
            "  2. Re-run this notebook from the beginning\n",
            "  3. Verify GPU detection in Step 4\n",
            "  4. Then proceed to notebook 01_Setup_Environment.ipynb\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Installation configuration\n",
        "# Set this to True to actually install PyTorch with CUDA\n",
        "# Set to False to just see what would be installed\n",
        "INSTALL_PYTORCH = True  # Change to True to install\n",
        "\n",
        "# Determine which pip to use\n",
        "# We want to use the virtual environment's pip if it exists\n",
        "# Otherwise use system pip\n",
        "venv_path = project_root / \".venv\"\n",
        "if venv_path.exists():\n",
        "    if platform.system() == \"Windows\":\n",
        "        pip_cmd = [str(venv_path / \"Scripts\" / \"pip\")]\n",
        "    else:\n",
        "        pip_cmd = [str(venv_path / \"bin\" / \"pip\")]\n",
        "    print(f\"Using virtual environment pip: {pip_cmd[0]}\")\n",
        "else:\n",
        "    pip_cmd = [sys.executable, \"-m\", \"pip\"]\n",
        "    print(f\"Using system pip: {' '.join(pip_cmd)}\")\n",
        "\n",
        "# Build installation command for CUDA 13.0\n",
        "# This is the correct command for DGX Spark\n",
        "# Why cu130? PyTorch 2.9+ supports CUDA 13.0, and DGX Spark uses CUDA 13.0\n",
        "cuda_index_url = \"https://download.pytorch.org/whl/cu130\"\n",
        "install_cmd = pip_cmd + [\n",
        "    \"install\",\n",
        "    \"--upgrade\",  # Upgrade if already installed\n",
        "    \"torch\",\n",
        "    \"torchvision\", \n",
        "    \"torchaudio\",\n",
        "    \"--index-url\", cuda_index_url\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PYTORCH INSTALLATION COMMAND\")\n",
        "print(\"=\" * 70)\n",
        "print(\" \".join(install_cmd))\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if INSTALL_PYTORCH:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"INSTALLING PyTorch with CUDA 13.0 support...\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"âš  This will take several minutes (downloading ~2GB of packages)\")\n",
        "    print(\"âš  After installation, RESTART THE KERNEL (Kernel â†’ Restart Kernel)\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Step 1: Uninstall existing PyTorch if CPU-only\n",
        "    # Why uninstall first? Prevents conflicts, ensures clean installation\n",
        "    if needs_reinstall:\n",
        "        print(\"\\nStep 1: Uninstalling CPU-only PyTorch...\")\n",
        "        uninstall_cmd = pip_cmd + [\"uninstall\", \"-y\", \"torch\", \"torchvision\", \"torchaudio\"]\n",
        "        result = subprocess.run(uninstall_cmd, capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"âœ“ CPU-only PyTorch uninstalled\")\n",
        "        else:\n",
        "            print(f\"âš  Uninstall had issues: {result.stderr}\")\n",
        "    \n",
        "    # Step 2: Install PyTorch with CUDA 13.0\n",
        "    print(\"\\nStep 2: Installing PyTorch with CUDA 13.0 support...\")\n",
        "    print(\"This downloads and installs:\")\n",
        "    print(\"  - torch (PyTorch core with CUDA 13.0)\")\n",
        "    print(\"  - torchvision (computer vision utilities)\")\n",
        "    print(\"  - torchaudio (audio processing utilities)\")\n",
        "    print(\"  - NVIDIA CUDA libraries (cudnn, cublas, etc.)\")\n",
        "    print(\"\\nDownloading... (this may take 5-10 minutes)\")\n",
        "    \n",
        "    result = subprocess.run(install_cmd, capture_output=False, text=True)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"âœ“ PyTorch with CUDA 13.0 installed successfully!\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"\\nâš  CRITICAL NEXT STEPS:\")\n",
        "        print(\"  1. Go to: Kernel â†’ Restart Kernel (or Kernel â†’ Restart & Clear Output)\")\n",
        "        print(\"  2. Re-run this notebook from the beginning\")\n",
        "        print(\"  3. Verify GPU detection in Step 4\")\n",
        "        print(\"  4. Then proceed to notebook 01_Setup_Environment.ipynb\")\n",
        "        print(\"=\" * 70)\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"âœ— Installation failed\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"  1. Check internet connection (needs to download ~2GB)\")\n",
        "        print(\"  2. Ensure you have enough disk space\")\n",
        "        print(\"  3. Try running in terminal:\")\n",
        "        print(f\"     {' '.join(install_cmd)}\")\n",
        "        print(\"=\" * 70)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"INSTALLATION NOT RUN (INSTALL_PYTORCH = False)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nTo install PyTorch with CUDA 13.0:\")\n",
        "    print(\"  1. Change INSTALL_PYTORCH = True in the cell above\")\n",
        "    print(\"  2. Re-run this cell\")\n",
        "    print(\"  3. After installation, restart kernel and re-run all cells\")\n",
        "    print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify GPU Detection\n",
        "\n",
        "After installing PyTorch with CUDA support and restarting the kernel, run this cell to verify that PyTorch can see and use your GPU.\n",
        "\n",
        "**What to expect:**\n",
        "- PyTorch version should show `+cu130` (not `+cpu`)\n",
        "- `torch.cuda.is_available()` should return `True`\n",
        "- GPU name should be `NVIDIA GB10` (or similar)\n",
        "- No warnings about compute capability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PYTORCH GPU VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "PyTorch Version: 2.9.1+cu130\n",
            "âœ“ CUDA Support: Yes (version 13.0)\n",
            "CUDA Available: True\n",
            "GPU Count: 1\n",
            "\n",
            "GPU 0: NVIDIA GB10\n",
            "  Total Memory: 119.7 GB\n",
            "  Compute Capability: 12.1\n",
            "  âœ“ This is the DGX Spark GB10 GPU!\n",
            "\n",
            "Testing GPU computation...\n",
            "âœ“ GPU computation test passed!\n",
            "  Test result: 567.8577\n",
            "  Device used: cuda:0\n",
            "\n",
            "======================================================================\n",
            "âœ“ SUCCESS: GPU is working with PyTorch!\n",
            "======================================================================\n",
            "\n",
            "You can now proceed to:\n",
            "  - Notebook 01_Setup_Environment.ipynb (full environment setup)\n",
            "  - Your GPU will be used for all PyTorch operations\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
            "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
            "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
            "    (8.0) - (12.0)\n",
            "    \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Verify PyTorch installation and GPU access\n",
        "# This should be run AFTER installing PyTorch and restarting the kernel\n",
        "# If you see errors, make sure you restarted the kernel after installation\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"PYTORCH GPU VERIFICATION\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Check PyTorch version\n",
        "    pytorch_version = torch.__version__\n",
        "    print(f\"\\nPyTorch Version: {pytorch_version}\")\n",
        "    \n",
        "    # Check if CUDA is compiled in\n",
        "    pytorch_has_cuda = torch.version.cuda is not None\n",
        "    pytorch_cuda_version = torch.version.cuda if pytorch_has_cuda else None\n",
        "    \n",
        "    if pytorch_has_cuda:\n",
        "        print(f\"âœ“ CUDA Support: Yes (version {pytorch_cuda_version})\")\n",
        "        \n",
        "        # Check if CUDA runtime is available\n",
        "        cuda_available = torch.cuda.is_available()\n",
        "        print(f\"CUDA Available: {cuda_available}\")\n",
        "        \n",
        "        if cuda_available:\n",
        "            # Get GPU information\n",
        "            device_count = torch.cuda.device_count()\n",
        "            print(f\"GPU Count: {device_count}\")\n",
        "            \n",
        "            if device_count > 0:\n",
        "                for i in range(device_count):\n",
        "                    gpu_name = torch.cuda.get_device_name(i)\n",
        "                    gpu_props = torch.cuda.get_device_properties(i)\n",
        "                    \n",
        "                    print(f\"\\nGPU {i}: {gpu_name}\")\n",
        "                    print(f\"  Total Memory: {gpu_props.total_memory / (1024**3):.1f} GB\")\n",
        "                    print(f\"  Compute Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
        "                    \n",
        "                    # Check if it's GB10 (DGX Spark)\n",
        "                    if \"GB10\" in gpu_name.upper():\n",
        "                        print(\"  âœ“ This is the DGX Spark GB10 GPU!\")\n",
        "                \n",
        "                # Test GPU computation\n",
        "                print(\"\\nTesting GPU computation...\")\n",
        "                try:\n",
        "                    # Create a small tensor on GPU to verify it works\n",
        "                    # This is a simple test: create random tensor, move to GPU, compute sum\n",
        "                    test_tensor = torch.randn(1000, 1000)\n",
        "                    test_tensor_gpu = test_tensor.cuda()  # Move to GPU\n",
        "                    result = test_tensor_gpu.sum()  # Compute on GPU\n",
        "                    print(f\"âœ“ GPU computation test passed!\")\n",
        "                    print(f\"  Test result: {result.item():.4f}\")\n",
        "                    print(f\"  Device used: {test_tensor_gpu.device}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"âœ— GPU computation test failed: {e}\")\n",
        "                    print(\"  This suggests a problem with GPU access\")\n",
        "                \n",
        "                print(\"\\n\" + \"=\" * 70)\n",
        "                print(\"âœ“ SUCCESS: GPU is working with PyTorch!\")\n",
        "                print(\"=\" * 70)\n",
        "                print(\"\\nYou can now proceed to:\")\n",
        "                print(\"  - Notebook 01_Setup_Environment.ipynb (full environment setup)\")\n",
        "                print(\"  - Your GPU will be used for all PyTorch operations\")\n",
        "                print(\"=\" * 70)\n",
        "            else:\n",
        "                print(\"\\nâš  No GPUs detected by PyTorch\")\n",
        "                print(\"  Check CUDA drivers: nvidia-smi\")\n",
        "        else:\n",
        "            print(\"\\nâš  CUDA not available\")\n",
        "            print(\"  Possible causes:\")\n",
        "            print(\"    - CUDA drivers not installed\")\n",
        "            print(\"    - CUDA version mismatch\")\n",
        "            print(\"    - GPU in use by another process\")\n",
        "            print(\"  Check: nvidia-smi\")\n",
        "    else:\n",
        "        print(f\"âœ— CUDA Support: No (CPU-only version)\")\n",
        "        print(\"\\nâš  PyTorch is still CPU-only!\")\n",
        "        print(\"  Did you:\")\n",
        "        print(\"    1. Run the installation cell (Step 3)?\")\n",
        "        print(\"    2. Restart the kernel after installation?\")\n",
        "        print(\"    3. Re-run all cells from the beginning?\")\n",
        "        print(\"\\n  If not, go back to Step 3 and install PyTorch with CUDA\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"âœ— PyTorch not installed\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nGo to Step 3 to install PyTorch with CUDA support\")\n",
        "except Exception as e:\n",
        "    print(f\"Error verifying PyTorch: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: System Information Summary\n",
        "\n",
        "This cell provides a comprehensive summary of your DGX Spark system configuration for reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DGX SPARK SYSTEM REPORT\n",
            "======================================================================\n",
            "\n",
            "ðŸ–¥ï¸  System Information:\n",
            "  Architecture: aarch64\n",
            "  Platform: Linux-6.11.0-1016-nvidia-aarch64-with-glibc2.39\n",
            "  Python: 3.12.3 (/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/bin/python)\n",
            "\n",
            "ðŸŽ® GPU Information:\n",
            "  Status: Not detected (run Step 1 cells to detect GPU)\n",
            "  Note: Run cells from Step 1 if you haven't already\n",
            "\n",
            "ðŸ”¥ PyTorch Information:\n",
            "  Version: 2.9.1+cu130\n",
            "  CUDA Compiled: Yes\n",
            "  CUDA Version: 13.0\n",
            "  CUDA Available: Yes\n",
            "  GPU Count: 1\n",
            "  GPU Name: NVIDIA GB10\n",
            "\n",
            "ðŸ“¦ Installation Status:\n",
            "  Status: Unknown (run Step 2 cells to check PyTorch installation)\n",
            "  Note: Run cells from Step 2 if you haven't already\n",
            "\n",
            "======================================================================\n",
            "Report complete!\n",
            "======================================================================\n",
            "\n",
            "Note: If some information is missing, make sure to run all cells\n",
            "      from Steps 1-4 in order for complete system information.\n"
          ]
        }
      ],
      "source": [
        "# Generate comprehensive system report\n",
        "# This documents your DGX Spark configuration for reference\n",
        "# Note: Re-import modules in case kernel was restarted\n",
        "import sys\n",
        "import platform\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DGX SPARK SYSTEM REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# System information\n",
        "print(\"\\nðŸ–¥ï¸  System Information:\")\n",
        "print(f\"  Architecture: {platform.machine()}\")\n",
        "print(f\"  Platform: {platform.platform()}\")\n",
        "print(f\"  Python: {sys.version.split()[0]} ({sys.executable})\")\n",
        "\n",
        "# GPU information (with defensive checks)\n",
        "# Check if variables from earlier cells exist\n",
        "gpu_detected = globals().get('gpu_detected', False)\n",
        "gpu_name = globals().get('gpu_name', None)\n",
        "cuda_version = globals().get('cuda_version', None)\n",
        "\n",
        "if gpu_detected and gpu_name:\n",
        "    print(f\"\\nðŸŽ® GPU Information:\")\n",
        "    print(f\"  GPU Name: {gpu_name}\")\n",
        "    if cuda_version:\n",
        "        print(f\"  System CUDA: {cuda_version}\")\n",
        "else:\n",
        "    print(f\"\\nðŸŽ® GPU Information:\")\n",
        "    print(f\"  Status: Not detected (run Step 1 cells to detect GPU)\")\n",
        "    print(f\"  Note: Run cells from Step 1 if you haven't already\")\n",
        "\n",
        "# PyTorch information\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"\\nðŸ”¥ PyTorch Information:\")\n",
        "    print(f\"  Version: {torch.__version__}\")\n",
        "    print(f\"  CUDA Compiled: {'Yes' if torch.version.cuda else 'No'}\")\n",
        "    if torch.version.cuda:\n",
        "        print(f\"  CUDA Version: {torch.version.cuda}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"  CUDA Available: Yes\")\n",
        "            print(f\"  GPU Count: {torch.cuda.device_count()}\")\n",
        "            if torch.cuda.device_count() > 0:\n",
        "                print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            print(f\"  CUDA Available: No\")\n",
        "    else:\n",
        "        print(f\"  Status: CPU-only (needs CUDA installation)\")\n",
        "except ImportError:\n",
        "    print(f\"\\nðŸ”¥ PyTorch Information:\")\n",
        "    print(f\"  Status: Not installed\")\n",
        "\n",
        "# Installation status (with defensive checks)\n",
        "needs_installation = globals().get('needs_installation', None)\n",
        "needs_reinstall = globals().get('needs_reinstall', None)\n",
        "\n",
        "print(f\"\\nðŸ“¦ Installation Status:\")\n",
        "if needs_installation is True:\n",
        "    print(f\"  PyTorch: Not installed\")\n",
        "    print(f\"  Action: Install with CUDA 13.0 support\")\n",
        "elif needs_reinstall is True:\n",
        "    print(f\"  PyTorch: Installed (CPU-only)\")\n",
        "    print(f\"  Action: Reinstall with CUDA 13.0 support\")\n",
        "elif needs_installation is False and needs_reinstall is False:\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"  PyTorch: Installed with CUDA âœ“\")\n",
        "            print(f\"  Action: Ready to use!\")\n",
        "        else:\n",
        "            print(f\"  PyTorch: Installed with CUDA (but GPU not accessible)\")\n",
        "            print(f\"  Action: Check CUDA drivers\")\n",
        "    except:\n",
        "        print(f\"  PyTorch: Unknown status\")\n",
        "else:\n",
        "    # Variables not set - cells from Step 2 weren't run\n",
        "    print(f\"  Status: Unknown (run Step 2 cells to check PyTorch installation)\")\n",
        "    print(f\"  Note: Run cells from Step 2 if you haven't already\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Report complete!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nNote: If some information is missing, make sure to run all cells\")\n",
        "print(\"      from Steps 1-4 in order for complete system information.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "**What We Accomplished:**\n",
        "1. âœ… Detected DGX Spark system (ARM64 + GB10 GPU)\n",
        "2. âœ… Checked current PyTorch installation\n",
        "3. âœ… Installed PyTorch with CUDA 13.0 support (if needed)\n",
        "4. âœ… Verified GPU detection and access\n",
        "5. âœ… Generated system configuration report\n",
        "\n",
        "**Key Points for DGX Spark:**\n",
        "- **Architecture**: ARM64 (aarch64) - different from x86_64\n",
        "- **GPU**: NVIDIA GB10 (Blackwell, compute capability 12.1)\n",
        "- **CUDA**: 13.0 - use `cu130` PyTorch index\n",
        "- **PyTorch**: 2.9+ has official DGX Spark support\n",
        "\n",
        "**Common Issues and Solutions:**\n",
        "\n",
        "1. **CPU-only PyTorch installed:**\n",
        "   - Solution: Uninstall and reinstall from `cu130` index\n",
        "   - Command: `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130`\n",
        "\n",
        "2. **GPU not detected after installation:**\n",
        "   - Solution: Restart kernel (Kernel â†’ Restart Kernel)\n",
        "   - Then re-run verification cell\n",
        "\n",
        "3. **Compute capability warnings:**\n",
        "   - These are informational (GB10 is 12.1, PyTorch supports up to 12.0)\n",
        "   - GPU will still work, may use compatibility mode\n",
        "   - Future PyTorch versions will fully support 12.1\n",
        "\n",
        "**Next Steps:**\n",
        "1. If GPU is working: Proceed to `01_Setup_Environment.ipynb`\n",
        "2. If issues persist: Check NVIDIA drivers (`nvidia-smi`)\n",
        "3. For optimal performance: Consider using NVIDIA's prebuilt containers\n",
        "\n",
        "**References:**\n",
        "- PyTorch CUDA 13.0 wheels: https://download.pytorch.org/whl/cu130/\n",
        "- DGX Spark documentation: NVIDIA Developer Forums\n",
        "- PyTorch DGX Spark support: https://discuss.pytorch.org/t/nvidia-dgx-spark-support/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
