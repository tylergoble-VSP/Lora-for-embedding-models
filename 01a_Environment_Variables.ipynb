{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01a Environment Variables (Hugging Face)\n",
        "\n",
        "This notebook validates Hugging Face access without using the CLI. It relies on the `HF_TOKEN` environment variable to authenticate and then performs a small, controlled download check.\n",
        "\n",
        "Run order:\n",
        "1. Set `HF_TOKEN` (and optional test model env vars).\n",
        "2. Validate token + account access.\n",
        "3. Run the model/tokenizer download test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Set `HF_TOKEN` (no CLI)\n",
        "\n",
        "You must create a Hugging Face access token and set it as an environment variable **before** running the auth check.\n",
        "\n",
        "### Option A: Set in your shell\n",
        "\n",
        "```bash\n",
        "export HF_TOKEN=\"your_hf_token_here\"\n",
        "```\n",
        "\n",
        "### Option B: Set inside the notebook (temporary)\n",
        "\n",
        "Use the next cell to paste your token (input is hidden). This sets `HF_TOKEN` for the current kernel only.\n",
        "\n",
        "Notes:\n",
        "- Make sure you have accepted any gated model licenses you plan to download.\n",
        "- Do **not** print or share your token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF_TOKEN not found in environment.\n",
            "HF_TOKEN loaded (masked): hf_Z...Ssdi\n"
          ]
        }
      ],
      "source": [
        "import os  # Python standard library (environment variables)\n",
        "from getpass import getpass  # Python standard library (hidden input)\n",
        "\n",
        "\n",
        "def get_hf_token() -> str:\n",
        "    \"\"\"Return a usable Hugging Face token from env or a hidden prompt.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the token is missing or empty after prompting.\n",
        "    \"\"\"\n",
        "    token = os.environ.get(\"HF_TOKEN\")\n",
        "    if token:\n",
        "        return token\n",
        "\n",
        "    print(\"HF_TOKEN not found in environment.\")\n",
        "    token = getpass(\"Paste HF_TOKEN (input hidden): \").strip()\n",
        "    if not token:\n",
        "        raise ValueError(\"HF_TOKEN is required to authenticate with Hugging Face.\")\n",
        "\n",
        "    # Set it for this kernel session only (not persisted to your shell).\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    return token\n",
        "\n",
        "\n",
        "hf_token = get_hf_token()\n",
        "print(\"HF_TOKEN loaded (masked):\", f\"{hf_token[:4]}...{hf_token[-4:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face authentication: ✓\n",
            "Logged in as: goblevsp\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import whoami  # huggingface_hub package (HF auth utilities)\n",
        "\n",
        "try:\n",
        "    user_info = whoami(token=hf_token)\n",
        "    print(\"Hugging Face authentication: ✓\")\n",
        "    print(f\"Logged in as: {user_info.get('name', 'Unknown')}\")\n",
        "except Exception as exc:\n",
        "    print(\"Hugging Face authentication: ✗\")\n",
        "    print(f\"Error: {exc}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Configure the download test\n",
        "\n",
        "To avoid hardcoding any model IDs, set a small, public model ID in your environment.\n",
        "\n",
        "### Option A: Set in your shell\n",
        "\n",
        "```bash\n",
        "export HF_TEST_MODEL_ID=\"<your-small-public-model-id>\"\n",
        "```\n",
        "\n",
        "If you use the shell approach, make sure you **launch Jupyter from the same shell** so the kernel inherits the environment variables.\n",
        "\n",
        "### Option B: Set inside the notebook (temporary)\n",
        "\n",
        "Use the next cell to set `HF_TEST_MODEL_ID` for this kernel session only.\n",
        "\n",
        "Optional:\n",
        "- Set `HF_DOWNLOAD_WEIGHTS=true` if you want to download full model weights.\n",
        "- If omitted, the test only downloads the model config + tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Look at the top of your VS Code and copy paste google/gemma-3-1b-it into that command!!!\n",
        "\n",
        "google/gemma-3-1b-it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF_TEST_MODEL_ID not found in environment.\n",
            "HF_TEST_MODEL_ID set for this kernel: google/gemma-3-1b-it\n"
          ]
        }
      ],
      "source": [
        "import os  # Python standard library (environment variables)\n",
        "\n",
        "\n",
        "def get_test_model_id() -> str:\n",
        "    \"\"\"Return a test model ID from env or prompt the user.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the model ID is missing or empty after prompting.\n",
        "    \"\"\"\n",
        "    model_id = os.environ.get(\"HF_TEST_MODEL_ID\")\n",
        "    if model_id:\n",
        "        return model_id\n",
        "\n",
        "    print(\"HF_TEST_MODEL_ID not found in environment.\")\n",
        "    model_id = input(\"Enter a small public model ID: \").strip()\n",
        "    if not model_id:\n",
        "        raise ValueError(\"HF_TEST_MODEL_ID is required for the download test.\")\n",
        "\n",
        "    # Set it for this kernel session only (not persisted to your shell).\n",
        "    os.environ[\"HF_TEST_MODEL_ID\"] = model_id\n",
        "    return model_id\n",
        "\n",
        "\n",
        "test_model_id = get_test_model_id()\n",
        "print(\"HF_TEST_MODEL_ID set for this kernel:\", test_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download test model_id: google/gemma-3-1b-it\n",
            "Download full weights: True\n",
            "Config + tokenizer download: ✓\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Gemma3TextModel were not initialized from the model checkpoint at google/gemma-3-1b-it and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.post_feedforward_layernorm.weight', 'layers.0.pre_feedforward_layernorm.weight', 'layers.0.self_attn.k_norm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_norm.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.post_feedforward_layernorm.weight', 'layers.1.pre_feedforward_layernorm.weight', 'layers.1.self_attn.k_norm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_norm.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.post_feedforward_layernorm.weight', 'layers.10.pre_feedforward_layernorm.weight', 'layers.10.self_attn.k_norm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_norm.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.post_feedforward_layernorm.weight', 'layers.11.pre_feedforward_layernorm.weight', 'layers.11.self_attn.k_norm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_norm.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.post_feedforward_layernorm.weight', 'layers.12.pre_feedforward_layernorm.weight', 'layers.12.self_attn.k_norm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_norm.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.post_feedforward_layernorm.weight', 'layers.13.pre_feedforward_layernorm.weight', 'layers.13.self_attn.k_norm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_norm.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.post_feedforward_layernorm.weight', 'layers.14.pre_feedforward_layernorm.weight', 'layers.14.self_attn.k_norm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_norm.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.post_feedforward_layernorm.weight', 'layers.15.pre_feedforward_layernorm.weight', 'layers.15.self_attn.k_norm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_norm.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.post_feedforward_layernorm.weight', 'layers.16.pre_feedforward_layernorm.weight', 'layers.16.self_attn.k_norm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_norm.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.post_feedforward_layernorm.weight', 'layers.17.pre_feedforward_layernorm.weight', 'layers.17.self_attn.k_norm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_norm.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.post_feedforward_layernorm.weight', 'layers.18.pre_feedforward_layernorm.weight', 'layers.18.self_attn.k_norm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_norm.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.post_feedforward_layernorm.weight', 'layers.19.pre_feedforward_layernorm.weight', 'layers.19.self_attn.k_norm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_norm.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.post_feedforward_layernorm.weight', 'layers.2.pre_feedforward_layernorm.weight', 'layers.2.self_attn.k_norm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_norm.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.post_feedforward_layernorm.weight', 'layers.20.pre_feedforward_layernorm.weight', 'layers.20.self_attn.k_norm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_norm.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.post_feedforward_layernorm.weight', 'layers.21.pre_feedforward_layernorm.weight', 'layers.21.self_attn.k_norm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_norm.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.post_feedforward_layernorm.weight', 'layers.22.pre_feedforward_layernorm.weight', 'layers.22.self_attn.k_norm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_norm.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.post_feedforward_layernorm.weight', 'layers.23.pre_feedforward_layernorm.weight', 'layers.23.self_attn.k_norm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_norm.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.post_feedforward_layernorm.weight', 'layers.24.pre_feedforward_layernorm.weight', 'layers.24.self_attn.k_norm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_norm.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.post_feedforward_layernorm.weight', 'layers.25.pre_feedforward_layernorm.weight', 'layers.25.self_attn.k_norm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_norm.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.post_feedforward_layernorm.weight', 'layers.3.pre_feedforward_layernorm.weight', 'layers.3.self_attn.k_norm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_norm.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.post_feedforward_layernorm.weight', 'layers.4.pre_feedforward_layernorm.weight', 'layers.4.self_attn.k_norm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_norm.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.post_feedforward_layernorm.weight', 'layers.5.pre_feedforward_layernorm.weight', 'layers.5.self_attn.k_norm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_norm.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.post_feedforward_layernorm.weight', 'layers.6.pre_feedforward_layernorm.weight', 'layers.6.self_attn.k_norm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_norm.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.post_feedforward_layernorm.weight', 'layers.7.pre_feedforward_layernorm.weight', 'layers.7.self_attn.k_norm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_norm.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.post_feedforward_layernorm.weight', 'layers.8.pre_feedforward_layernorm.weight', 'layers.8.self_attn.k_norm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_norm.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.post_feedforward_layernorm.weight', 'layers.9.pre_feedforward_layernorm.weight', 'layers.9.self_attn.k_norm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_norm.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights download: ✓\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoModel, AutoTokenizer  # transformers package (model utilities)\n",
        "\n",
        "model_id = globals().get(\"test_model_id\") or os.environ.get(\"HF_TEST_MODEL_ID\")\n",
        "if not model_id:\n",
        "    raise ValueError(\n",
        "        \"HF_TEST_MODEL_ID is not set. Set it to a small public model ID and rerun.\"\n",
        "    )\n",
        "\n",
        "download_weights = os.environ.get(\"HF_DOWNLOAD_WEIGHTS\", \"true\").lower() == \"true\"\n",
        "\n",
        "print(f\"Download test model_id: {model_id}\")\n",
        "print(f\"Download full weights: {download_weights}\")\n",
        "\n",
        "try:\n",
        "    config = AutoConfig.from_pretrained(model_id, token=hf_token)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "    print(\"Config + tokenizer download: ✓\")\n",
        "\n",
        "    if download_weights:\n",
        "        _ = AutoModel.from_pretrained(model_id, token=hf_token)\n",
        "        print(\"Model weights download: ✓\")\n",
        "    else:\n",
        "        print(\"Model weights download: skipped (set HF_DOWNLOAD_WEIGHTS=true to enable)\")\n",
        "except Exception as exc:\n",
        "    print(\"Download test failed.\")\n",
        "    print(f\"Error: {exc}\")\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
