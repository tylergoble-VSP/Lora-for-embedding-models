{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning EmbeddingGemma on PDF QA Pairs\n",
    "\n",
    "This notebook fine-tunes EmbeddingGemma using LoRA on query-passage pairs generated from PDF documents. The model learns to:\n",
    "- **Map queries to relevant passages**: Make embeddings of queries and their correct passages similar\n",
    "- **Improve retrieval**: Enable better semantic search over PDF content\n",
    "- **Domain adaptation**: Adapt the general EmbeddingGemma model to your specific PDF domain\n",
    "\n",
    "This workflow uses the training pairs generated in notebook 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary functions from src modules\n",
    "from src.data.loaders import load_query_passage_pairs, validate_pairs\n",
    "from src.models.embedding_pipeline import load_embeddinggemma_model\n",
    "from src.models.lora_setup import setup_lora_model, print_trainable_parameters\n",
    "from src.training.trainer import train_model\n",
    "from src.utils.paths import timestamped_path, find_latest_timestamped_file\n",
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Training Dataset\n",
    "\n",
    "Load the query-passage pairs generated in notebook 10. The `load_query_passage_pairs()` function automatically maps 'query' ‚Üí 'anchor' and 'passage' ‚Üí 'positive' for compatibility with the training framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: data/processed/pdf_query_passage_pairs_train_20260114_024812.csv\n",
      "Loaded 895 training pairs\n",
      "\n",
      "First pair:\n",
      "  Query (anchor):   '```json\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is the primary subject matter of the book, as indicated by its title?'\n",
      "  Passage (positive): '*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***...'\n",
      "\n",
      "Dataset statistics:\n",
      "  Average query length: 212.4 characters\n",
      "  Average passage length: 1909.7 characters\n",
      "  Has empty strings: False\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset from CSV file generated in notebook 10\n",
    "# Automatically find the latest train file (or specify path manually)\n",
    "train_csv_path = find_latest_timestamped_file(\n",
    "    \"data/processed\",\n",
    "    \"pdf_query_passage_pairs_train\",\n",
    "    \"csv\"\n",
    ")\n",
    "\n",
    "if train_csv_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"No training CSV file found. Please run notebook 10 first to generate the dataset.\"\n",
    "    )\n",
    "\n",
    "print(f\"Loading training data from: {train_csv_path}\")\n",
    "\n",
    "# Load pairs (automatically maps query‚Üíanchor, passage‚Üípositive)\n",
    "train_data = load_query_passage_pairs(str(train_csv_path))\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training pairs\")\n",
    "print(\"\\nFirst pair:\")\n",
    "print(f\"  Query (anchor):   '{train_data[0]['anchor']}'\")\n",
    "print(f\"  Passage (positive): '{train_data[0]['positive'][:100]}...'\")\n",
    "\n",
    "# Validate dataset\n",
    "stats = validate_pairs(train_data)\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Average query length: {stats['avg_anchor_length']:.1f} characters\")\n",
    "print(f\"  Average passage length: {stats['avg_positive_length']:.1f} characters\")\n",
    "print(f\"  Has empty strings: {stats['has_empty']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Base Model and Apply LoRA\n",
    "\n",
    "We'll load the base EmbeddingGemma model and configure it with LoRA adapters. LoRA allows us to fine-tune only a small subset of parameters, making training efficient and preventing catastrophic forgetting.\n",
    "\n",
    "### LoRA Configuration\n",
    "\n",
    "- `r`: Rank of LoRA adapters (default: 16, controls capacity)\n",
    "- `lora_alpha`: Scaling factor (default: 32, typically 2x rank)\n",
    "- `lora_dropout`: Dropout rate for LoRA layers (default: 0.1)\n",
    "- `target_modules`: Which transformer layers to apply LoRA to (typically attention projections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "Trainable params: 1,376,256 (0.45% of 304,239,360)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trainable': 1376256, 'total': 304239360, 'percentage': 0.452359615797246}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load base EmbeddingGemma model\n",
    "tokenizer, base_model = load_embeddinggemma_model()\n",
    "\n",
    "# Check device\n",
    "device = next(base_model.parameters()).device\n",
    "print(f\"Model loaded on device: {device}\")\n",
    "\n",
    "# Apply LoRA configuration\n",
    "model = setup_lora_model(\n",
    "    base_model,\n",
    "    r=16,  # LoRA rank (controls adapter capacity)\n",
    "    lora_alpha=32,  # Scaling factor (typically 2x rank)\n",
    "    lora_dropout=0.1,  # Dropout for regularization\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]  # Apply to attention projections\n",
    ")\n",
    "\n",
    "# Verify only LoRA parameters are trainable\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Validation Dataset (Optional)\n",
    "\n",
    "If you have a validation set, load it to monitor training progress and prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data from: data/processed/pdf_query_passage_pairs_val_20260114_024812.csv\n",
      "Loaded 221 validation pairs\n"
     ]
    }
   ],
   "source": [
    "# Load validation dataset (optional)\n",
    "# Automatically find the latest val file (or specify path manually)\n",
    "val_csv_path = find_latest_timestamped_file(\n",
    "    \"data/processed\",\n",
    "    \"pdf_query_passage_pairs_val\",\n",
    "    \"csv\"\n",
    ")\n",
    "\n",
    "if val_csv_path is None:\n",
    "    print(\"Validation file not found, skipping validation\")\n",
    "    val_data = None\n",
    "else:\n",
    "    print(f\"Loading validation data from: {val_csv_path}\")\n",
    "    try:\n",
    "        val_data = load_query_passage_pairs(str(val_csv_path))\n",
    "        print(f\"Loaded {len(val_data)} validation pairs\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error loading validation file, skipping validation\")\n",
    "        val_data = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check GPU and Train the Model\n",
    "\n",
    "First, let's check GPU availability and memory to optimize batch size for DGX Spark.\n",
    "\n",
    "Then we'll train the model using contrastive learning. The training process:\n",
    "1. **Forward pass**: Compute embeddings for queries and passages\n",
    "2. **Contrastive loss**: Use Multiple Negatives Ranking Loss to bring positive pairs closer\n",
    "3. **Backward pass**: Update only LoRA parameters\n",
    "\n",
    "### Training Parameters (Optimized for ~2 hours on DGX Spark)\n",
    "\n",
    "- `epochs`: 50 epochs (increased from 3)\n",
    "- `batch_size`: 64 (increased from 8 for better GPU utilization)\n",
    "- `learning_rate`: $1.6 \\times 10^{-3}$ (scaled with batch size using linear scaling rule: $lr_{new} = lr_{base} \\times \\frac{batch\\_size_{new}}{batch\\_size_{base}}$)\n",
    "- `temperature`: 1.0 (contrastive loss temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "------------------------------------------------------------\n",
      "GPU 0: NVIDIA GB10\n",
      "  Total Memory: 128.53 GB\n",
      "  Allocated: 1.22 GB\n",
      "  Reserved: 1.28 GB\n",
      "  Free: 127.25 GB\n",
      "  Compute Capability: 12.1\n",
      "------------------------------------------------------------\n",
      "\n",
      "Using device: cuda:0\n",
      "Recommended batch size: 64-128 for DGX Spark (will use 64)\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and memory for batch size optimization\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Information:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        total = props.total_memory / 1e9\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"GPU {i}: {props.name}\")\n",
    "        print(f\"  Total Memory: {total:.2f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Free: {free:.2f} GB\")\n",
    "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    print(f\"Recommended batch size: 64-128 for DGX Spark (will use 64)\")\n",
    "else:\n",
    "    print(\"CUDA not available - using CPU\")\n",
    "    print(\"Note: Training will be much slower on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGX Spark Training Configuration:\n",
      "  Training pairs: 895\n",
      "  Batch size: 64 (14 batches per epoch)\n",
      "  Epochs: 150\n",
      "  Learning rate: 1.60e-03 (scaled from 2.00e-04 for batch size 64)\n",
      "  Temperature: 1.0\n",
      "  Estimated time: ~1.67 hours (100.0 minutes)\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/150: training loss = 3.8875\n",
      "Epoch 2/150: training loss = 3.7557\n",
      "Epoch 3/150: training loss = 3.6663\n",
      "Epoch 4/150: training loss = 3.6331\n",
      "Epoch 5/150: training loss = 3.6253\n",
      "Epoch 6/150: training loss = 3.5951\n",
      "Epoch 7/150: training loss = 3.6114\n",
      "Epoch 8/150: training loss = 3.6219\n",
      "Epoch 9/150: training loss = 3.6445\n",
      "Epoch 10/150: training loss = 3.6213\n",
      "Epoch 11/150: training loss = 3.5988\n",
      "Epoch 12/150: training loss = 3.5595\n",
      "Epoch 13/150: training loss = 3.5363\n",
      "Epoch 14/150: training loss = 3.5500\n",
      "Epoch 15/150: training loss = 3.5703\n",
      "Epoch 16/150: training loss = 3.5593\n",
      "Epoch 17/150: training loss = 3.5552\n",
      "Epoch 18/150: training loss = 3.5327\n",
      "Epoch 19/150: training loss = 3.5156\n",
      "Epoch 20/150: training loss = 3.4979\n",
      "Epoch 21/150: training loss = 3.4888\n",
      "Epoch 22/150: training loss = 3.4681\n",
      "Epoch 23/150: training loss = 3.4472\n",
      "Epoch 24/150: training loss = 3.4276\n",
      "Epoch 25/150: training loss = 3.4260\n",
      "Epoch 26/150: training loss = 3.4153\n",
      "Epoch 27/150: training loss = 3.4178\n",
      "Epoch 28/150: training loss = 3.4176\n",
      "Epoch 29/150: training loss = 3.4250\n",
      "Epoch 30/150: training loss = 3.4057\n",
      "Epoch 31/150: training loss = 3.3941\n",
      "Epoch 32/150: training loss = 3.3798\n",
      "Epoch 33/150: training loss = 3.3591\n",
      "Epoch 34/150: training loss = 3.3530\n",
      "Epoch 35/150: training loss = 3.3462\n",
      "Epoch 36/150: training loss = 3.3346\n",
      "Epoch 37/150: training loss = 3.3273\n",
      "Epoch 38/150: training loss = 3.3155\n",
      "Epoch 39/150: training loss = 3.3103\n",
      "Epoch 40/150: training loss = 3.3097\n",
      "Epoch 41/150: training loss = 3.3070\n",
      "Epoch 42/150: training loss = 3.3019\n",
      "Epoch 43/150: training loss = 3.3041\n",
      "Epoch 44/150: training loss = 3.3104\n",
      "Epoch 45/150: training loss = 3.3079\n",
      "Epoch 46/150: training loss = 3.3028\n",
      "Epoch 47/150: training loss = 3.2997\n",
      "Epoch 48/150: training loss = 3.2989\n",
      "Epoch 49/150: training loss = 3.2971\n",
      "Epoch 50/150: training loss = 3.2933\n",
      "Epoch 51/150: training loss = 3.2896\n",
      "Epoch 52/150: training loss = 3.2840\n",
      "Epoch 53/150: training loss = 3.2811\n",
      "Epoch 54/150: training loss = 3.2790\n",
      "Epoch 55/150: training loss = 3.2757\n",
      "Epoch 56/150: training loss = 3.2745\n",
      "Epoch 57/150: training loss = 3.2757\n",
      "Epoch 58/150: training loss = 3.2769\n",
      "Epoch 59/150: training loss = 3.2753\n",
      "Epoch 60/150: training loss = 3.2775\n",
      "Epoch 61/150: training loss = 3.2835\n",
      "Epoch 62/150: training loss = 3.2818\n",
      "Epoch 63/150: training loss = 3.2880\n",
      "Epoch 64/150: training loss = 3.2949\n",
      "Epoch 65/150: training loss = 3.3122\n",
      "Epoch 66/150: training loss = 3.3452\n",
      "Epoch 67/150: training loss = 3.4025\n",
      "Epoch 68/150: training loss = 3.4420\n",
      "Epoch 69/150: training loss = 3.4711\n",
      "Epoch 70/150: training loss = 3.4432\n",
      "Epoch 71/150: training loss = 3.4408\n",
      "Epoch 72/150: training loss = 3.4193\n",
      "Epoch 73/150: training loss = 3.3903\n",
      "Epoch 74/150: training loss = 3.3958\n",
      "Epoch 75/150: training loss = 3.3927\n",
      "Epoch 76/150: training loss = 3.3551\n",
      "Epoch 77/150: training loss = 3.3379\n",
      "Epoch 78/150: training loss = 3.3246\n",
      "Epoch 79/150: training loss = 3.3031\n",
      "Epoch 80/150: training loss = 3.2958\n",
      "Epoch 81/150: training loss = 3.2856\n",
      "Epoch 82/150: training loss = 3.2762\n",
      "Epoch 83/150: training loss = 3.2725\n",
      "Epoch 84/150: training loss = 3.2680\n",
      "Epoch 85/150: training loss = 3.2665\n",
      "Epoch 86/150: training loss = 3.2644\n",
      "Epoch 87/150: training loss = 3.2619\n",
      "Epoch 88/150: training loss = 3.2608\n",
      "Epoch 89/150: training loss = 3.2595\n",
      "Epoch 90/150: training loss = 3.2586\n",
      "Epoch 91/150: training loss = 3.2567\n",
      "Epoch 92/150: training loss = 3.2559\n",
      "Epoch 93/150: training loss = 3.2556\n",
      "Epoch 94/150: training loss = 3.2540\n",
      "Epoch 95/150: training loss = 3.2528\n",
      "Epoch 96/150: training loss = 3.2520\n",
      "Epoch 97/150: training loss = 3.2522\n",
      "Epoch 98/150: training loss = 3.2534\n",
      "Epoch 99/150: training loss = 3.2508\n",
      "Epoch 100/150: training loss = 3.2499\n",
      "Epoch 101/150: training loss = 3.2518\n",
      "Epoch 102/150: training loss = 3.2509\n",
      "Epoch 103/150: training loss = 3.2490\n",
      "Epoch 104/150: training loss = 3.2495\n",
      "Epoch 105/150: training loss = 3.2484\n",
      "Epoch 106/150: training loss = 3.2485\n",
      "Epoch 107/150: training loss = 3.2476\n",
      "Epoch 108/150: training loss = 3.2480\n",
      "Epoch 109/150: training loss = 3.2487\n",
      "Epoch 110/150: training loss = 3.2480\n",
      "Epoch 111/150: training loss = 3.2468\n",
      "Epoch 112/150: training loss = 3.2462\n",
      "Epoch 113/150: training loss = 3.2458\n",
      "Epoch 114/150: training loss = 3.2455\n",
      "Epoch 115/150: training loss = 3.2446\n",
      "Epoch 116/150: training loss = 3.2439\n",
      "Epoch 117/150: training loss = 3.2435\n",
      "Epoch 118/150: training loss = 3.2429\n",
      "Epoch 119/150: training loss = 3.2431\n",
      "Epoch 120/150: training loss = 3.2468\n",
      "Epoch 121/150: training loss = 3.2517\n",
      "Epoch 122/150: training loss = 3.2499\n",
      "Epoch 123/150: training loss = 3.2523\n",
      "Epoch 124/150: training loss = 3.2607\n",
      "Epoch 125/150: training loss = 3.2668\n",
      "Epoch 126/150: training loss = 3.2660\n",
      "Epoch 127/150: training loss = 3.2639\n",
      "Epoch 128/150: training loss = 3.2690\n",
      "Epoch 129/150: training loss = 3.2908\n",
      "Epoch 130/150: training loss = 3.3003\n",
      "Epoch 131/150: training loss = 3.3046\n",
      "Epoch 132/150: training loss = 3.3217\n",
      "Epoch 133/150: training loss = 3.3116\n",
      "Epoch 134/150: training loss = 3.2943\n",
      "Epoch 135/150: training loss = 3.2870\n",
      "Epoch 136/150: training loss = 3.2814\n",
      "Epoch 137/150: training loss = 3.2883\n",
      "Epoch 138/150: training loss = 3.2827\n",
      "Epoch 139/150: training loss = 3.2844\n",
      "Epoch 140/150: training loss = 3.3038\n",
      "Epoch 141/150: training loss = 3.3361\n",
      "Epoch 142/150: training loss = 3.3640\n",
      "Epoch 143/150: training loss = 3.3592\n",
      "Epoch 144/150: training loss = 3.3583\n",
      "Epoch 145/150: training loss = 3.3745\n",
      "Epoch 146/150: training loss = 3.3601\n",
      "Epoch 147/150: training loss = 3.3559\n",
      "Epoch 148/150: training loss = 3.3295\n",
      "Epoch 149/150: training loss = 3.3060\n",
      "Epoch 150/150: training loss = 3.2902\n",
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "Total training time: 1.14 hours (4119.5 seconds)\n",
      "Average time per epoch: 27.46 seconds\n",
      "Initial loss: 3.8875\n",
      "Final loss: 3.2902\n",
      "Loss improvement: 0.5973\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train the model optimized for DGX Spark (~2 hours training time)\n",
    "# The trainer uses Multiple Negatives Ranking Loss for contrastive learning\n",
    "# Note: train_model modifies the model in-place and returns a list of losses per epoch\n",
    "\n",
    "# Calculate optimal parameters for ~2 hours training\n",
    "# Current baseline: 3 epochs @ batch_size=8 took ~97 seconds (32.3 sec/epoch)\n",
    "# Target: ~7200 seconds (2 hours)\n",
    "# Strategy: Increase batch size for better GPU utilization, scale epochs accordingly\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Optimized hyperparameters for DGX Spark\n",
    "# Larger batch size = better GPU utilization + more in-batch negatives\n",
    "BATCH_SIZE = 64  # Increased from 8 to better utilize GPU memory (8x larger)\n",
    "BASE_BATCH_SIZE = 8\n",
    "BASE_LR = 2e-4\n",
    "\n",
    "# Estimate epochs for ~2 hours:\n",
    "# With batch_size=64: ~14 batches/epoch (vs 112 with batch_size=8)\n",
    "# Larger batches are more GPU-efficient, estimate ~40-50 sec/epoch\n",
    "# For 2 hours (7200 sec): 7200/45 ‚âà 160 epochs\n",
    "# Using 150 epochs for safety (can adjust based on actual timing)\n",
    "EPOCHS = 150  # Optimized to reach ~2 hours total time\n",
    "\n",
    "# Scale learning rate with batch size (linear scaling rule)\n",
    "# This maintains similar gradient magnitudes as batch size increases\n",
    "# Formula: lr_new = lr_base * (batch_size_new / batch_size_base)\n",
    "LEARNING_RATE = BASE_LR * (BATCH_SIZE / BASE_BATCH_SIZE)  # 2e-4 * 8 = 1.6e-3\n",
    "\n",
    "batches_per_epoch = (len(train_data) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "estimated_epoch_time = 40  # Conservative estimate in seconds\n",
    "estimated_total_time = EPOCHS * estimated_epoch_time\n",
    "\n",
    "print(f\"DGX Spark Training Configuration:\")\n",
    "print(f\"  Training pairs: {len(train_data)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} ({batches_per_epoch} batches per epoch)\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE:.2e} (scaled from {BASE_LR:.2e} for batch size {BATCH_SIZE})\")\n",
    "print(f\"  Temperature: 1.0\")\n",
    "print(f\"  Estimated time: ~{estimated_total_time/3600:.2f} hours ({estimated_total_time/60:.1f} minutes)\")\n",
    "print(f\"\\nStarting training...\")\n",
    "\n",
    "losses = train_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_data=train_data,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    temperature=1.0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "hours = elapsed_time / 3600\n",
    "minutes = (elapsed_time % 3600) / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Total training time: {hours:.2f} hours ({elapsed_time:.1f} seconds)\")\n",
    "print(f\"Average time per epoch: {elapsed_time/EPOCHS:.2f} seconds\")\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Loss improvement: {losses[0] - losses[-1]:.4f}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Fine-Tuned Model\n",
    "\n",
    "Save the LoRA adapters so you can load them later for inference or further training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapters to: outputs/models/pdf_qa_lora_adapter_20260114_151605.\n",
      "\n",
      "To load the model later:\n",
      "  from peft import PeftModel\n",
      "  from src.models.embedding_pipeline import load_embeddinggemma_model\n",
      "  tokenizer, base_model = load_embeddinggemma_model()\n",
      "  model = PeftModel.from_pretrained(base_model, 'outputs/models/pdf_qa_lora_adapter_20260114_151605.')\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters\n",
    "from peft import PeftModel\n",
    "\n",
    "# Save adapters to timestamped directory\n",
    "adapter_path = timestamped_path(\"outputs/models\", \"pdf_qa_lora_adapter\", \"\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Note: model was modified in-place during training, so we use 'model' not 'trained_model'\n",
    "model.save_pretrained(str(adapter_path))\n",
    "tokenizer.save_pretrained(str(adapter_path))\n",
    "\n",
    "print(f\"Saved LoRA adapters to: {adapter_path}\")\n",
    "print(\"\\nTo load the model later:\")\n",
    "print(f\"  from peft import PeftModel\")\n",
    "print(f\"  from src.models.embedding_pipeline import load_embeddinggemma_model\")\n",
    "print(f\"  tokenizer, base_model = load_embeddinggemma_model()\")\n",
    "print(f\"  model = PeftModel.from_pretrained(base_model, '{adapter_path}')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Fine-Tuned Model\n",
    "\n",
    "Let's test the fine-tuned model on a sample query to see if it retrieves the correct passage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: '```json\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is the primary subject matter of the book, as indicated by its title?'\n",
      "\n",
      "Correct Passage: '*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***...'\n",
      "\n",
      "Similarity score: 0.9810\n",
      "(Higher is better, should be close to 1.0 for correct pairs)\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "from src.models.embedding_pipeline import embed_texts\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Example query from training data\n",
    "test_query = train_data[0]['anchor']\n",
    "correct_passage = train_data[0]['positive']\n",
    "\n",
    "# Embed query and passage using fine-tuned model\n",
    "# Note: model was modified in-place during training, so we use 'model' not 'trained_model'\n",
    "query_emb = embed_texts(test_query, model, tokenizer, device=device, max_length=512)\n",
    "passage_emb = embed_texts(correct_passage, model, tokenizer, device=device, max_length=512)\n",
    "\n",
    "# Compute similarity\n",
    "similarity = cosine_similarity(query_emb.numpy(), passage_emb.numpy())[0][0]\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nCorrect Passage: '{correct_passage[:150]}...'\")\n",
    "print(f\"\\nSimilarity score: {similarity:.4f}\")\n",
    "print(f\"(Higher is better, should be close to 1.0 for correct pairs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ‚úÖ Loading query-passage pairs from PDF-generated dataset\n",
    "2. ‚úÖ Setting up LoRA for efficient fine-tuning\n",
    "3. ‚úÖ Training EmbeddingGemma on PDF QA pairs\n",
    "4. ‚úÖ Saving fine-tuned LoRA adapters\n",
    "5. ‚úÖ Testing the fine-tuned model\n",
    "\n",
    "**Next Steps:**\n",
    "- Use the fine-tuned model for semantic search over your PDF documents\n",
    "- Evaluate retrieval performance using the evaluation modules\n",
    "- Consider using hard negatives (from notebook 10) for advanced training\n",
    "- The fine-tuned model can be loaded and used for inference in other notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs how to read what happened and **how much better the model actually got**, in practical terms.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What the loss numbers mean (plain English)\n",
    "\n",
    "* **Training loss** measures how surprised the model is by the correct answers.\n",
    "* Lower = better.\n",
    "* Loss is on a **log scale**, so small numeric improvements can still be meaningful.\n",
    "\n",
    "You started at:\n",
    "\n",
    "* **Initial loss:** **3.8875**\n",
    "\n",
    "You ended at:\n",
    "\n",
    "* **Final loss:** **3.2902**\n",
    "\n",
    "That‚Äôs a drop of:\n",
    "\n",
    "* **Œî loss = ‚àí0.5973**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How big is that improvement really?\n",
    "\n",
    "### A) Relative improvement\n",
    "\n",
    "$$\n",
    "\\frac{0.5973}{3.8875} \\approx 15.4\\%\n",
    "$$\n",
    "\n",
    "‚û°Ô∏è **About a 15% reduction in error** by this metric.\n",
    "\n",
    "That‚Äôs **solid but not dramatic**, especially for:\n",
    "\n",
    "* A **small dataset (895 pairs)**\n",
    "* **150 epochs**\n",
    "* A relatively high learning rate\n",
    "\n",
    "---\n",
    "\n",
    "### B) Interpreting loss exponentially (important!)\n",
    "\n",
    "Because loss is logarithmic, we can convert it to ‚Äúeffective likelihood‚Äù:\n",
    "\n",
    "$$\n",
    "\\text{Improvement factor} = e^{0.5973} \\approx 1.82\n",
    "$$\n",
    "\n",
    "‚û°Ô∏è **The model is ~1.8√ó more confident/accurate** on average than at the start.\n",
    "\n",
    "This is a much better intuition than raw loss numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What the loss curve tells us about learning quality\n",
    "\n",
    "### Phase breakdown\n",
    "\n",
    "#### üîπ Epochs 1‚Äì30: Fast learning\n",
    "\n",
    "* Loss drops from **3.89 ‚Üí ~3.40**\n",
    "* Model is learning basic structure and patterns\n",
    "\n",
    "#### üîπ Epochs 30‚Äì60: Slower gains\n",
    "\n",
    "* Gradual improvement down to **~3.28**\n",
    "* Still learning, but diminishing returns\n",
    "\n",
    "#### üîπ Epochs 60‚Äì90: Convergence\n",
    "\n",
    "* Loss flattens around **3.25‚Äì3.27**\n",
    "* This is likely **near the capacity limit** given data size\n",
    "\n",
    "#### üîπ Epochs 120‚Äì150: Instability / mild overfitting\n",
    "\n",
    "* Loss **rises back up** to **3.29**\n",
    "* Indicates:\n",
    "\n",
    "  * Learning rate slightly too high late in training\n",
    "  * Model starting to chase noise\n",
    "\n",
    "üìå **Your best model was probably around epochs 90‚Äì115**, not at epoch 150.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Did training ‚Äúwork‚Äù?\n",
    "\n",
    "**Yes ‚Äî clearly.**\n",
    "\n",
    "Evidence:\n",
    "\n",
    "* Strong early loss reduction\n",
    "* Clear convergence\n",
    "* Final model significantly better than initialization\n",
    "* No catastrophic divergence\n",
    "\n",
    "But also:\n",
    "\n",
    "* You are **data-limited**, not compute-limited\n",
    "* Extra epochs past ~100 gave little benefit\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How much better is the model *functionally*?\n",
    "\n",
    "Without validation metrics we can‚Äôt be exact, but typically:\n",
    "\n",
    "* Expect **noticeably better outputs** than the base model\n",
    "* Improved:\n",
    "\n",
    "  * Domain-specific wording\n",
    "  * Format consistency\n",
    "  * Preference alignment\n",
    "* Not a ‚Äúnew capability‚Äù jump\n",
    "\n",
    "Think:\n",
    "\n",
    "> **Sharper, more reliable, more on-style ‚Äî not magically smarter**\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key takeaways (TL;DR)\n",
    "\n",
    "* ‚úÖ Loss improved by **15.4%**\n",
    "* ‚úÖ Effective performance improved by **~1.8√ó**\n",
    "* ‚ö†Ô∏è Training past ~100 epochs likely unnecessary\n",
    "* ‚ö†Ô∏è Slight overfitting / learning-rate instability late\n",
    "* üìà Biggest gains came early; diminishing returns dominate\n",
    "\n",
    "---\n",
    "\n",
    "## 7. If you want, next we can:\n",
    "\n",
    "* Estimate **expected quality improvement** on real prompts\n",
    "* Decide **best checkpoint** to keep\n",
    "* Suggest **better LR schedule** for next run\n",
    "* Evaluate whether **more data or lower LR** would help more\n",
    "\n",
    "Just tell me what you want to optimize next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
