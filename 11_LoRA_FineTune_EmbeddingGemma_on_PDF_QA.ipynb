{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Fine-Tuning EmbeddingGemma on PDF QA Pairs\n",
        "\n",
        "This notebook fine-tunes EmbeddingGemma using LoRA on query-passage pairs generated from PDF documents. The model learns to:\n",
        "- **Map queries to relevant passages**: Make embeddings of queries and their correct passages similar\n",
        "- **Improve retrieval**: Enable better semantic search over PDF content\n",
        "- **Domain adaptation**: Adapt the general EmbeddingGemma model to your specific PDF domain\n",
        "\n",
        "This workflow uses the training pairs generated in notebook 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary functions from src modules\n",
        "from src.data.loaders import load_query_passage_pairs, validate_pairs\n",
        "from src.models.embedding_pipeline import load_embeddinggemma_model\n",
        "from src.models.lora_setup import setup_lora_model, print_trainable_parameters\n",
        "from src.training.trainer import train_model\n",
        "from src.utils.paths import timestamped_path\n",
        "import torch\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Training Dataset\n",
        "\n",
        "Load the query-passage pairs generated in notebook 10. The `load_query_passage_pairs()` function automatically maps 'query' → 'anchor' and 'passage' → 'positive' for compatibility with the training framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training dataset from CSV file generated in notebook 10\n",
        "# Replace with the actual path to your train CSV file\n",
        "train_csv_path = \"data/processed/pdf_query_passage_pairs_train_YYYYMMDD_HHMMSS.csv\"  # Update this path\n",
        "\n",
        "# Load pairs (automatically maps query→anchor, passage→positive)\n",
        "train_data = load_query_passage_pairs(train_csv_path)\n",
        "\n",
        "print(f\"Loaded {len(train_data)} training pairs\")\n",
        "print(\"\\nFirst pair:\")\n",
        "print(f\"  Query (anchor):   '{train_data[0]['anchor']}'\")\n",
        "print(f\"  Passage (positive): '{train_data[0]['positive'][:100]}...'\")\n",
        "\n",
        "# Validate dataset\n",
        "stats = validate_pairs(train_data)\n",
        "print(f\"\\nDataset statistics:\")\n",
        "print(f\"  Average query length: {stats['avg_anchor_length']:.1f} characters\")\n",
        "print(f\"  Average passage length: {stats['avg_positive_length']:.1f} characters\")\n",
        "print(f\"  Has empty strings: {stats['has_empty']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Base Model and Apply LoRA\n",
        "\n",
        "We'll load the base EmbeddingGemma model and configure it with LoRA adapters. LoRA allows us to fine-tune only a small subset of parameters, making training efficient and preventing catastrophic forgetting.\n",
        "\n",
        "### LoRA Configuration\n",
        "\n",
        "- `r`: Rank of LoRA adapters (default: 16, controls capacity)\n",
        "- `lora_alpha`: Scaling factor (default: 32, typically 2x rank)\n",
        "- `lora_dropout`: Dropout rate for LoRA layers (default: 0.1)\n",
        "- `target_modules`: Which transformer layers to apply LoRA to (typically attention projections)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base EmbeddingGemma model\n",
        "tokenizer, base_model = load_embeddinggemma_model()\n",
        "\n",
        "# Check device\n",
        "device = next(base_model.parameters()).device\n",
        "print(f\"Model loaded on device: {device}\")\n",
        "\n",
        "# Apply LoRA configuration\n",
        "model = setup_lora_model(\n",
        "    base_model,\n",
        "    r=16,  # LoRA rank (controls adapter capacity)\n",
        "    lora_alpha=32,  # Scaling factor (typically 2x rank)\n",
        "    lora_dropout=0.1,  # Dropout for regularization\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]  # Apply to attention projections\n",
        ")\n",
        "\n",
        "# Verify only LoRA parameters are trainable\n",
        "print_trainable_parameters(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Validation Dataset (Optional)\n",
        "\n",
        "If you have a validation set, load it to monitor training progress and prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load validation dataset (optional)\n",
        "# Replace with the actual path to your val CSV file\n",
        "val_csv_path = \"data/processed/pdf_query_passage_pairs_val_YYYYMMDD_HHMMSS.csv\"  # Update this path\n",
        "\n",
        "try:\n",
        "    val_data = load_query_passage_pairs(val_csv_path)\n",
        "    print(f\"Loaded {len(val_data)} validation pairs\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Validation file not found, skipping validation\")\n",
        "    val_data = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train the Model\n",
        "\n",
        "Now we'll train the model using contrastive learning. The training process:\n",
        "1. **Forward pass**: Compute embeddings for queries and passages\n",
        "2. **Contrastive loss**: Use Multiple Negatives Ranking Loss to bring positive pairs closer\n",
        "3. **Backward pass**: Update only LoRA parameters\n",
        "4. **Validation**: Monitor performance on validation set (if available)\n",
        "\n",
        "### Training Parameters\n",
        "\n",
        "- `epochs`: Number of training epochs\n",
        "- `batch_size`: Batch size for training\n",
        "- `learning_rate`: Learning rate for optimizer\n",
        "- `max_length`: Maximum sequence length for tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "# The trainer uses Multiple Negatives Ranking Loss for contrastive learning\n",
        "trained_model = train_model(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_data=train_data,\n",
        "    val_data=val_data,  # None if validation not available\n",
        "    epochs=3,  # Adjust based on dataset size\n",
        "    batch_size=8,  # Adjust based on GPU memory\n",
        "    learning_rate=2e-4,  # Learning rate for LoRA fine-tuning\n",
        "    max_length=512,  # Should match chunk max_tokens\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Save Fine-Tuned Model\n",
        "\n",
        "Save the LoRA adapters so you can load them later for inference or further training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters\n",
        "from peft import PeftModel\n",
        "\n",
        "# Save adapters to timestamped directory\n",
        "adapter_path = timestamped_path(\"outputs/models\", \"pdf_qa_lora_adapter\", \"\")\n",
        "adapter_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "trained_model.save_pretrained(str(adapter_path))\n",
        "tokenizer.save_pretrained(str(adapter_path))\n",
        "\n",
        "print(f\"Saved LoRA adapters to: {adapter_path}\")\n",
        "print(\"\\nTo load the model later:\")\n",
        "print(f\"  from peft import PeftModel\")\n",
        "print(f\"  from src.models.embedding_pipeline import load_embeddinggemma_model\")\n",
        "print(f\"  tokenizer, base_model = load_embeddinggemma_model()\")\n",
        "print(f\"  model = PeftModel.from_pretrained(base_model, '{adapter_path}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test Fine-Tuned Model\n",
        "\n",
        "Let's test the fine-tuned model on a sample query to see if it retrieves the correct passage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "from src.models.embedding_pipeline import embed_texts\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Example query from training data\n",
        "test_query = train_data[0]['anchor']\n",
        "correct_passage = train_data[0]['positive']\n",
        "\n",
        "# Embed query and passage using fine-tuned model\n",
        "query_emb = embed_texts(test_query, trained_model, tokenizer, device=device, max_length=512)\n",
        "passage_emb = embed_texts(correct_passage, trained_model, tokenizer, device=device, max_length=512)\n",
        "\n",
        "# Compute similarity\n",
        "similarity = cosine_similarity(query_emb.numpy(), passage_emb.numpy())[0][0]\n",
        "\n",
        "print(f\"Query: '{test_query}'\")\n",
        "print(f\"\\nCorrect Passage: '{correct_passage[:150]}...'\")\n",
        "print(f\"\\nSimilarity score: {similarity:.4f}\")\n",
        "print(f\"(Higher is better, should be close to 1.0 for correct pairs)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. ✅ Loading query-passage pairs from PDF-generated dataset\n",
        "2. ✅ Setting up LoRA for efficient fine-tuning\n",
        "3. ✅ Training EmbeddingGemma on PDF QA pairs\n",
        "4. ✅ Saving fine-tuned LoRA adapters\n",
        "5. ✅ Testing the fine-tuned model\n",
        "\n",
        "**Next Steps:**\n",
        "- Use the fine-tuned model for semantic search over your PDF documents\n",
        "- Evaluate retrieval performance using the evaluation modules\n",
        "- Consider using hard negatives (from notebook 10) for advanced training\n",
        "- The fine-tuned model can be loaded and used for inference in other notebooks\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
