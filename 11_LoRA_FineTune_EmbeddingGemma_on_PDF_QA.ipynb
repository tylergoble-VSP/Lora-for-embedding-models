{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Fine-Tuning EmbeddingGemma on PDF QA Pairs\n",
        "\n",
        "This notebook fine-tunes EmbeddingGemma using LoRA on query-passage pairs generated from PDF documents. The model learns to:\n",
        "- **Map queries to relevant passages**: Make embeddings of queries and their correct passages similar\n",
        "- **Improve retrieval**: Enable better semantic search over PDF content\n",
        "- **Domain adaptation**: Adapt the general EmbeddingGemma model to your specific PDF domain\n",
        "\n",
        "This workflow uses the training pairs generated in notebook 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary functions from src modules\n",
        "from src.data.loaders import load_query_passage_pairs, validate_pairs\n",
        "from src.models.embedding_pipeline import load_embeddinggemma_model\n",
        "from src.models.lora_setup import setup_lora_model, print_trainable_parameters\n",
        "from src.training.trainer import train_model\n",
        "from src.utils.paths import timestamped_path, find_latest_timestamped_file\n",
        "import torch\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Training Dataset\n",
        "\n",
        "Load the query-passage pairs generated in notebook 10. The `load_query_passage_pairs()` function automatically maps 'query' → 'anchor' and 'passage' → 'positive' for compatibility with the training framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data from: data/processed/pdf_query_passage_pairs_train_20260114_024812.csv\n",
            "Loaded 895 training pairs\n",
            "\n",
            "First pair:\n",
            "  Query (anchor):   '```json\n",
            "[\n",
            "  {\n",
            "    \"question\": \"What is the primary subject matter of the book, as indicated by its title?'\n",
            "  Passage (positive): '*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***...'\n",
            "\n",
            "Dataset statistics:\n",
            "  Average query length: 212.4 characters\n",
            "  Average passage length: 1909.7 characters\n",
            "  Has empty strings: False\n"
          ]
        }
      ],
      "source": [
        "# Load training dataset from CSV file generated in notebook 10\n",
        "# Automatically find the latest train file (or specify path manually)\n",
        "train_csv_path = find_latest_timestamped_file(\n",
        "    \"data/processed\",\n",
        "    \"pdf_query_passage_pairs_train\",\n",
        "    \"csv\"\n",
        ")\n",
        "\n",
        "if train_csv_path is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"No training CSV file found. Please run notebook 10 first to generate the dataset.\"\n",
        "    )\n",
        "\n",
        "print(f\"Loading training data from: {train_csv_path}\")\n",
        "\n",
        "# Load pairs (automatically maps query→anchor, passage→positive)\n",
        "train_data = load_query_passage_pairs(str(train_csv_path))\n",
        "\n",
        "print(f\"Loaded {len(train_data)} training pairs\")\n",
        "print(\"\\nFirst pair:\")\n",
        "print(f\"  Query (anchor):   '{train_data[0]['anchor']}'\")\n",
        "print(f\"  Passage (positive): '{train_data[0]['positive'][:100]}...'\")\n",
        "\n",
        "# Validate dataset\n",
        "stats = validate_pairs(train_data)\n",
        "print(f\"\\nDataset statistics:\")\n",
        "print(f\"  Average query length: {stats['avg_anchor_length']:.1f} characters\")\n",
        "print(f\"  Average passage length: {stats['avg_positive_length']:.1f} characters\")\n",
        "print(f\"  Has empty strings: {stats['has_empty']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Base Model and Apply LoRA\n",
        "\n",
        "We'll load the base EmbeddingGemma model and configure it with LoRA adapters. LoRA allows us to fine-tune only a small subset of parameters, making training efficient and preventing catastrophic forgetting.\n",
        "\n",
        "### LoRA Configuration\n",
        "\n",
        "- `r`: Rank of LoRA adapters (default: 16, controls capacity)\n",
        "- `lora_alpha`: Scaling factor (default: 32, typically 2x rank)\n",
        "- `lora_dropout`: Dropout rate for LoRA layers (default: 0.1)\n",
        "- `target_modules`: Which transformer layers to apply LoRA to (typically attention projections)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
            "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
            "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
            "    (8.0) - (12.0)\n",
            "    \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on device: cuda:0\n",
            "Trainable params: 1,376,256 (0.45% of 304,239,360)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'trainable': 1376256, 'total': 304239360, 'percentage': 0.452359615797246}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load base EmbeddingGemma model\n",
        "tokenizer, base_model = load_embeddinggemma_model()\n",
        "\n",
        "# Check device\n",
        "device = next(base_model.parameters()).device\n",
        "print(f\"Model loaded on device: {device}\")\n",
        "\n",
        "# Apply LoRA configuration\n",
        "model = setup_lora_model(\n",
        "    base_model,\n",
        "    r=16,  # LoRA rank (controls adapter capacity)\n",
        "    lora_alpha=32,  # Scaling factor (typically 2x rank)\n",
        "    lora_dropout=0.1,  # Dropout for regularization\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]  # Apply to attention projections\n",
        ")\n",
        "\n",
        "# Verify only LoRA parameters are trainable\n",
        "print_trainable_parameters(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Validation Dataset (Optional)\n",
        "\n",
        "If you have a validation set, load it to monitor training progress and prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading validation data from: data/processed/pdf_query_passage_pairs_val_20260114_024812.csv\n",
            "Loaded 221 validation pairs\n"
          ]
        }
      ],
      "source": [
        "# Load validation dataset (optional)\n",
        "# Automatically find the latest val file (or specify path manually)\n",
        "val_csv_path = find_latest_timestamped_file(\n",
        "    \"data/processed\",\n",
        "    \"pdf_query_passage_pairs_val\",\n",
        "    \"csv\"\n",
        ")\n",
        "\n",
        "if val_csv_path is None:\n",
        "    print(\"Validation file not found, skipping validation\")\n",
        "    val_data = None\n",
        "else:\n",
        "    print(f\"Loading validation data from: {val_csv_path}\")\n",
        "    try:\n",
        "        val_data = load_query_passage_pairs(str(val_csv_path))\n",
        "        print(f\"Loaded {len(val_data)} validation pairs\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error loading validation file, skipping validation\")\n",
        "        val_data = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Check GPU and Train the Model\n",
        "\n",
        "First, let's check GPU availability and memory to optimize batch size for DGX Spark.\n",
        "\n",
        "Then we'll train the model using contrastive learning. The training process:\n",
        "1. **Forward pass**: Compute embeddings for queries and passages\n",
        "2. **Contrastive loss**: Use Multiple Negatives Ranking Loss to bring positive pairs closer\n",
        "3. **Backward pass**: Update only LoRA parameters\n",
        "\n",
        "### Training Parameters (Optimized for ~2 hours on DGX Spark)\n",
        "\n",
        "- `epochs`: 50 epochs (increased from 3)\n",
        "- `batch_size`: 64 (increased from 8 for better GPU utilization)\n",
        "- `learning_rate`: 1.6e-3 (scaled with batch size using linear scaling rule)\n",
        "- `temperature`: 1.0 (contrastive loss temperature)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability and memory for batch size optimization\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Information:\")\n",
        "    print(\"-\" * 60)\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
        "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
        "        total = props.total_memory / 1e9\n",
        "        free = total - reserved\n",
        "        \n",
        "        print(f\"GPU {i}: {props.name}\")\n",
        "        print(f\"  Total Memory: {total:.2f} GB\")\n",
        "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
        "        print(f\"  Free: {free:.2f} GB\")\n",
        "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "    print(f\"Recommended batch size: 64-128 for DGX Spark (will use 64)\")\n",
        "else:\n",
        "    print(\"CUDA not available - using CPU\")\n",
        "    print(\"Note: Training will be much slower on CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: training loss = 1.9081\n",
            "Epoch 2/3: training loss = 1.8330\n",
            "Epoch 3/3: training loss = 1.7800\n",
            "Training completed!\n",
            "Initial loss: 1.9081\n",
            "Final loss: 1.7800\n"
          ]
        }
      ],
      "source": [
        "# Train the model optimized for DGX Spark (~2 hours training time)\n",
        "# The trainer uses Multiple Negatives Ranking Loss for contrastive learning\n",
        "# Note: train_model modifies the model in-place and returns a list of losses per epoch\n",
        "\n",
        "# Calculate optimal parameters for ~2 hours training\n",
        "# Current baseline: 3 epochs @ batch_size=8 took ~97 seconds (32.3 sec/epoch)\n",
        "# Target: ~7200 seconds (2 hours)\n",
        "# Strategy: Increase batch size for better GPU utilization, scale epochs accordingly\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Optimized hyperparameters for DGX Spark\n",
        "# Larger batch size = better GPU utilization + more in-batch negatives\n",
        "BATCH_SIZE = 64  # Increased from 8 to better utilize GPU memory (8x larger)\n",
        "BASE_BATCH_SIZE = 8\n",
        "BASE_LR = 2e-4\n",
        "\n",
        "# Estimate epochs for ~2 hours:\n",
        "# With batch_size=64: ~14 batches/epoch (vs 112 with batch_size=8)\n",
        "# Larger batches are more GPU-efficient, estimate ~40-50 sec/epoch\n",
        "# For 2 hours (7200 sec): 7200/45 ≈ 160 epochs\n",
        "# Using 150 epochs for safety (can adjust based on actual timing)\n",
        "EPOCHS = 150  # Optimized to reach ~2 hours total time\n",
        "\n",
        "# Scale learning rate with batch size (linear scaling rule)\n",
        "# This maintains similar gradient magnitudes as batch size increases\n",
        "# Formula: lr_new = lr_base * (batch_size_new / batch_size_base)\n",
        "LEARNING_RATE = BASE_LR * (BATCH_SIZE / BASE_BATCH_SIZE)  # 2e-4 * 8 = 1.6e-3\n",
        "\n",
        "batches_per_epoch = (len(train_data) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "estimated_epoch_time = 40  # Conservative estimate in seconds\n",
        "estimated_total_time = EPOCHS * estimated_epoch_time\n",
        "\n",
        "print(f\"DGX Spark Training Configuration:\")\n",
        "print(f\"  Training pairs: {len(train_data)}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} ({batches_per_epoch} batches per epoch)\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE:.2e} (scaled from {BASE_LR:.2e} for batch size {BATCH_SIZE})\")\n",
        "print(f\"  Temperature: 1.0\")\n",
        "print(f\"  Estimated time: ~{estimated_total_time/3600:.2f} hours ({estimated_total_time/60:.1f} minutes)\")\n",
        "print(f\"\\nStarting training...\")\n",
        "\n",
        "losses = train_model(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_data=train_data,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    temperature=1.0,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours = elapsed_time / 3600\n",
        "minutes = (elapsed_time % 3600) / 60\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Total training time: {hours:.2f} hours ({elapsed_time:.1f} seconds)\")\n",
        "print(f\"Average time per epoch: {elapsed_time/EPOCHS:.2f} seconds\")\n",
        "print(f\"Initial loss: {losses[0]:.4f}\")\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "print(f\"Loss improvement: {losses[0] - losses[-1]:.4f}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Save Fine-Tuned Model\n",
        "\n",
        "Save the LoRA adapters so you can load them later for inference or further training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved LoRA adapters to: outputs/models/pdf_qa_lora_adapter_20260114_131041.\n",
            "\n",
            "To load the model later:\n",
            "  from peft import PeftModel\n",
            "  from src.models.embedding_pipeline import load_embeddinggemma_model\n",
            "  tokenizer, base_model = load_embeddinggemma_model()\n",
            "  model = PeftModel.from_pretrained(base_model, 'outputs/models/pdf_qa_lora_adapter_20260114_131041.')\n"
          ]
        }
      ],
      "source": [
        "# Save LoRA adapters\n",
        "from peft import PeftModel\n",
        "\n",
        "# Save adapters to timestamped directory\n",
        "adapter_path = timestamped_path(\"outputs/models\", \"pdf_qa_lora_adapter\", \"\")\n",
        "adapter_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Note: model was modified in-place during training, so we use 'model' not 'trained_model'\n",
        "model.save_pretrained(str(adapter_path))\n",
        "tokenizer.save_pretrained(str(adapter_path))\n",
        "\n",
        "print(f\"Saved LoRA adapters to: {adapter_path}\")\n",
        "print(\"\\nTo load the model later:\")\n",
        "print(f\"  from peft import PeftModel\")\n",
        "print(f\"  from src.models.embedding_pipeline import load_embeddinggemma_model\")\n",
        "print(f\"  tokenizer, base_model = load_embeddinggemma_model()\")\n",
        "print(f\"  model = PeftModel.from_pretrained(base_model, '{adapter_path}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test Fine-Tuned Model\n",
        "\n",
        "Let's test the fine-tuned model on a sample query to see if it retrieves the correct passage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: '```json\n",
            "[\n",
            "  {\n",
            "    \"question\": \"What is the primary subject matter of the book, as indicated by its title?'\n",
            "\n",
            "Correct Passage: '*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***...'\n",
            "\n",
            "Similarity score: 0.5533\n",
            "(Higher is better, should be close to 1.0 for correct pairs)\n"
          ]
        }
      ],
      "source": [
        "# Test the fine-tuned model\n",
        "from src.models.embedding_pipeline import embed_texts\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Example query from training data\n",
        "test_query = train_data[0]['anchor']\n",
        "correct_passage = train_data[0]['positive']\n",
        "\n",
        "# Embed query and passage using fine-tuned model\n",
        "# Note: model was modified in-place during training, so we use 'model' not 'trained_model'\n",
        "query_emb = embed_texts(test_query, model, tokenizer, device=device, max_length=512)\n",
        "passage_emb = embed_texts(correct_passage, model, tokenizer, device=device, max_length=512)\n",
        "\n",
        "# Compute similarity\n",
        "similarity = cosine_similarity(query_emb.numpy(), passage_emb.numpy())[0][0]\n",
        "\n",
        "print(f\"Query: '{test_query}'\")\n",
        "print(f\"\\nCorrect Passage: '{correct_passage[:150]}...'\")\n",
        "print(f\"\\nSimilarity score: {similarity:.4f}\")\n",
        "print(f\"(Higher is better, should be close to 1.0 for correct pairs)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. ✅ Loading query-passage pairs from PDF-generated dataset\n",
        "2. ✅ Setting up LoRA for efficient fine-tuning\n",
        "3. ✅ Training EmbeddingGemma on PDF QA pairs\n",
        "4. ✅ Saving fine-tuned LoRA adapters\n",
        "5. ✅ Testing the fine-tuned model\n",
        "\n",
        "**Next Steps:**\n",
        "- Use the fine-tuned model for semantic search over your PDF documents\n",
        "- Evaluate retrieval performance using the evaluation modules\n",
        "- Consider using hard negatives (from notebook 10) for advanced training\n",
        "- The fine-tuned model can be loaded and used for inference in other notebooks\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
