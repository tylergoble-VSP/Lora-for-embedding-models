{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Environment Setup and Verification\n",
        "\n",
        "This notebook sets up the Python environment, installs dependencies, and verifies that all required components (Python, CUDA, GPU, Hugging Face) are properly configured.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
            "Python executable: /home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Python executable: {sys.executable}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create and Activate Virtual Environment\n",
        "\n",
        "Run the following commands in your terminal to set up the virtual environment:\n",
        "\n",
        "```bash\n",
        "python -m venv .venv\n",
        "source .venv/bin/activate  # On Linux/Mac\n",
        "# or\n",
        ".venv\\Scripts\\activate  # On Windows\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# Note: Run this cell to install required packages\n",
        "# !pip install -q torch transformers peft pandas numpy scikit-learn matplotlib seaborn pytest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check CUDA and GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jan 20 19:08:08 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GB10                    On  |   0000000F:01:00.0 Off |                  N/A |\n",
            "| N/A   38C    P8              5W /  N/A  | Not Supported          |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A            7701      G   /usr/lib/xorg/Xorg                      118MiB |\n",
            "|    0   N/A  N/A            7845      G   /usr/bin/gnome-shell                    207MiB |\n",
            "|    0   N/A  N/A            9058      G   /usr/bin/gnome-control-center            35MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability using nvidia-smi\n",
        "# This confirms the DGX or CUDA environment is active and recognized.\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.1+cu130\n",
            "CUDA available: True\n",
            "CUDA version: 13.0\n",
            "GPU device: NVIDIA GB10\n",
            "GPU memory: 128.53 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
            "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
            "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
            "    (8.0) - (12.0)\n",
            "    \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check PyTorch CUDA availability\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"CUDA not available - will use CPU (slower)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Hugging Face Authentication\n",
        "\n",
        "The EmbeddingGemma model is gated and requires accepting the license on Hugging Face. Make sure you:\n",
        "1. Have a Hugging Face account\n",
        "2. Have accepted the model license: https://huggingface.co/google/embeddinggemma-300m\n",
        "3. Are logged in using `huggingface-cli login` or have set `HF_TOKEN` environment variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in as: goblevsp\n",
            "Hugging Face authentication: ✓\n"
          ]
        }
      ],
      "source": [
        "# Check Hugging Face login status\n",
        "import os\n",
        "from huggingface_hub import whoami\n",
        "\n",
        "try:\n",
        "    user_info = whoami()\n",
        "    print(f\"Logged in as: {user_info.get('name', 'Unknown')}\")\n",
        "    print(\"Hugging Face authentication: ✓\")\n",
        "except Exception as e:\n",
        "    print(f\"Hugging Face authentication: ✗\")\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nPlease run: huggingface-cli login\")\n",
        "    print(\"Or set HF_TOKEN environment variable\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Package Installations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify all required packages are installed\n",
        "required_packages = {\n",
        "    \"torch\": \"PyTorch\",\n",
        "    \"transformers\": \"Transformers\",\n",
        "    \"peft\": \"PEFT\",\n",
        "    \"pandas\": \"Pandas\",\n",
        "    \"numpy\": \"NumPy\",\n",
        "    \"sklearn\": \"Scikit-learn\",\n",
        "    \"matplotlib\": \"Matplotlib\",\n",
        "    \"pytest\": \"Pytest\"\n",
        "}\n",
        "\n",
        "print(\"Package Verification:\")\n",
        "print(\"-\" * 40)\n",
        "for package, name in required_packages.items():\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"✓ {name} ({package})\")\n",
        "    except ImportError:\n",
        "        print(f\"✗ {name} ({package}) - NOT INSTALLED\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Report Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate environment report\n",
        "import platform\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"ENVIRONMENT REPORT\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "try:\n",
        "    from transformers import __version__ as transformers_version\n",
        "    print(f\"Transformers: {transformers_version}\")\n",
        "except:\n",
        "    print(\"Transformers: Not installed\")\n",
        "\n",
        "try:\n",
        "    from peft import __version__ as peft_version\n",
        "    print(f\"PEFT: {peft_version}\")\n",
        "except:\n",
        "    print(\"PEFT: Not installed\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Environment setup complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
