{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation and Analysis\n",
        "\n",
        "This notebook evaluates the fine-tuned model by computing similarity metrics, ranking accuracy, and analyzing hard negatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import functions from the scripts directory\n",
        "from src.data.loaders import load_toy_dataset\n",
        "from src.models.embedding_pipeline import load_embeddinggemma_model, embed_texts\n",
        "from src.models.lora_setup import setup_lora_model\n",
        "from src.evaluation.similarity_metrics import (\n",
        "    compute_similarity_matrix,\n",
        "    rank_positives_for_anchors,\n",
        "    compute_accuracy,\n",
        "    compute_mean_reciprocal_rank,\n",
        "    analyze_hard_negatives\n",
        ")\n",
        "import pandas as pd\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Fine-Tuned Model\n",
        "\n",
        "Load the model with LoRA adapters. If you've trained a model, load it here. Otherwise, we'll use a freshly configured model for demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with LoRA (for demonstration, using base model with LoRA setup)\n",
        "# In practice, you would load a trained model:\n",
        "# from peft import PeftModel\n",
        "# base_model = AutoModel.from_pretrained('google/embeddinggemma-300m')\n",
        "# model = PeftModel.from_pretrained(base_model, 'outputs/models/embeddinggemma_lora_YYYYMMDD_HHMMSS')\n",
        "\n",
        "tokenizer, base_model = load_embeddinggemma_model()\n",
        "model = setup_lora_model(base_model, r=16, lora_alpha=32, lora_dropout=0.1)\n",
        "\n",
        "print(\"Model loaded for evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Data\n",
        "\n",
        "We'll evaluate on the same toy dataset to see how well the model distinguishes pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "train_data = load_toy_dataset()\n",
        "\n",
        "# Extract anchor and positive texts\n",
        "anchor_texts = [item[\"anchor\"] for item in train_data]\n",
        "positive_texts = [item[\"positive\"] for item in train_data]\n",
        "\n",
        "print(f\"Evaluating on {len(anchor_texts)} anchor-positive pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Embeddings\n",
        "\n",
        "Generate embeddings for all anchors and positives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute embeddings\n",
        "anchor_embeddings = embed_texts(anchor_texts, model, tokenizer)\n",
        "positive_embeddings = embed_texts(positive_texts, model, tokenizer)\n",
        "\n",
        "print(f\"Anchor embeddings shape: {anchor_embeddings.shape}\")\n",
        "print(f\"Positive embeddings shape: {positive_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Similarity Matrix\n",
        "\n",
        "See how similar each anchor is to each positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute similarity matrix\n",
        "sim_matrix = compute_similarity_matrix(anchor_embeddings, positive_embeddings)\n",
        "\n",
        "# Display as DataFrame\n",
        "sim_df = pd.DataFrame(\n",
        "    sim_matrix.numpy(),\n",
        "    index=[f\"Anchor {i+1}\" for i in range(len(anchor_texts))],\n",
        "    columns=[f\"Positive {i+1}\" for i in range(len(positive_texts))]\n",
        ")\n",
        "\n",
        "print(\"Similarity Matrix (Anchor × Positive):\")\n",
        "print(sim_df.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ranking Analysis\n",
        "\n",
        "For each anchor, see how the positives are ranked by similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get rankings for each anchor\n",
        "rankings, similarities = rank_positives_for_anchors(anchor_embeddings, positive_embeddings)\n",
        "\n",
        "# Display rankings\n",
        "for i, anchor in enumerate(anchor_texts):\n",
        "    print(f\"\\nAnchor {i+1}: '{anchor}'\")\n",
        "    print(\"Rankings (most similar first):\")\n",
        "    for rank_pos, pos_idx in enumerate(rankings[i]):\n",
        "        pos_idx_val = pos_idx.item()\n",
        "        sim = similarities[i, pos_idx_val].item()\n",
        "        marker = \"✓\" if pos_idx_val == i else \" \"\n",
        "        print(f\"  {marker} Rank {rank_pos+1}: Positive {pos_idx_val+1} (sim={sim:.3f}) - '{positive_texts[pos_idx_val]}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Metrics\n",
        "\n",
        "Calculate accuracy and Mean Reciprocal Rank (MRR).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute accuracy (correct positive ranked #1)\n",
        "accuracy = compute_accuracy(anchor_embeddings, positive_embeddings)\n",
        "\n",
        "# Compute MRR\n",
        "mrr = compute_mean_reciprocal_rank(anchor_embeddings, positive_embeddings)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Accuracy (correct positive at rank 1): {accuracy:.2%}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {mrr:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hard Negative Analysis\n",
        "\n",
        "Identify pairs that are incorrectly similar (hard negatives).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find hard negatives (incorrect pairs with high similarity)\n",
        "hard_negatives = analyze_hard_negatives(\n",
        "    anchor_texts,\n",
        "    positive_texts,\n",
        "    anchor_embeddings,\n",
        "    positive_embeddings,\n",
        "    threshold=0.3  # Consider pairs with similarity > 0.3 as hard negatives\n",
        ")\n",
        "\n",
        "if hard_negatives:\n",
        "    print(f\"Found {len(hard_negatives)} hard negative pairs:\")\n",
        "    for i, hn in enumerate(hard_negatives[:5], 1):  # Show top 5\n",
        "        print(f\"\\n{i}. Similarity: {hn['similarity']:.3f}\")\n",
        "        print(f\"   Anchor: '{hn['anchor']}'\")\n",
        "        print(f\"   Incorrect Positive: '{hn['incorrect_positive']}'\")\n",
        "else:\n",
        "    print(\"No hard negatives found (all incorrect pairs have low similarity)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
