{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Configuration\n",
        "\n",
        "This notebook configures LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning of EmbeddingGemma. LoRA allows us to fine-tune the model by training only a small number of additional parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import functions from the scripts directory\n",
        "from src.models.embedding_pipeline import load_embeddinggemma_model\n",
        "from src.models.lora_setup import setup_lora_model, print_trainable_parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Base Model\n",
        "\n",
        "First, we load the base EmbeddingGemma model that we'll fine-tune.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the base model\n",
        "tokenizer, base_model = load_embeddinggemma_model()\n",
        "\n",
        "print(\"Base model loaded successfully\")\n",
        "print(f\"Model device: {next(base_model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure and Apply LoRA\n",
        "\n",
        "We'll configure LoRA with:\n",
        "- Rank (r=16): Controls the size of the adapter matrices\n",
        "- Alpha (32): Scaling factor, typically 2*r\n",
        "- Dropout (0.1): Regularization during training\n",
        "- Target modules: [\"q_proj\", \"k_proj\", \"v_proj\"] - attention projection layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup LoRA on the model\n",
        "# This freezes the base model and adds LoRA adapters\n",
        "model = setup_lora_model(\n",
        "    base_model,\n",
        "    r=16,              # LoRA rank\n",
        "    lora_alpha=32,     # Scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]  # Attention projection layers\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters applied successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Trainable Parameters\n",
        "\n",
        "After applying LoRA, only a small fraction of parameters should be trainable (typically < 1% of the total).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print trainable parameter statistics\n",
        "stats = print_trainable_parameters(model)\n",
        "\n",
        "print(f\"\\nParameter Efficiency:\")\n",
        "print(f\"  - Only {stats['percentage']:.2f}% of parameters are trainable\")\n",
        "print(f\"  - This means we can fine-tune with much less memory and compute!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Model Still Works\n",
        "\n",
        "Let's make sure the model with LoRA still produces embeddings correctly (it should behave identically to the base model before training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test that the model still works\n",
        "from src.models.embedding_pipeline import embed_texts\n",
        "\n",
        "test_text = \"This is a test sentence.\"\n",
        "embedding = embed_texts(test_text, model, tokenizer)\n",
        "\n",
        "print(f\"Embedding shape: {embedding.shape}\")\n",
        "print(f\"Embedding norm: {embedding.norm().item():.4f} (should be ~1.0 for normalized embeddings)\")\n",
        "print(\"âœ“ Model with LoRA is working correctly!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
