{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question Generation and Dataset Building\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. **Generate Questions**: Create multiple questions per chunk using LLMs or heuristics\n",
        "2. **Build Training Pairs**: Create (query, passage) pairs for fine-tuning\n",
        "3. **Train/Val Split**: Split data with grouped splitting to avoid leakage\n",
        "4. **Hard Negative Mining**: Find similar but non-matching chunks for contrastive learning\n",
        "\n",
        "This workflow creates the training dataset needed for fine-tuning EmbeddingGemma on PDF content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/goble54/spark-dev-workspace/Lora-for-embedding-models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import functions from src modules\n",
        "from src.data.pdf_to_chunks_pipeline import pdf_to_chunks\n",
        "from src.data.question_generation_dataset import (\n",
        "    generate_questions_dataset,\n",
        "    train_val_split_pairs,\n",
        "    save_questions_dataset\n",
        ")\n",
        "from src.llm.question_generation import get_question_generator, QuestionGenConfig\n",
        "from src.pipelines.embed_chunks import embed_chunks\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load or Generate Chunks\n",
        "\n",
        "If you've already run notebook 09, you can load the chunks from the saved parquet file. Otherwise, we'll generate them here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 709 chunks to: data/processed/pdf_chunks_20260109_184228.parquet\n",
            "Saved metadata to: data/processed/pdf_chunks_metadata_20260109_184228.json\n",
            "Statistics: {'num_pages': 344, 'num_sections': 22, 'num_chunks': 709, 'total_tokens': 334063, 'avg_tokens_per_chunk': 471.17489421720734, 'min_tokens': 1, 'median_tokens': 490.0, 'max_tokens': 583}\n",
            "Loaded 709 chunks\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>section_id</th>\n",
              "      <th>section_title</th>\n",
              "      <th>page_start</th>\n",
              "      <th>page_end</th>\n",
              "      <th>token_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mobydick</td>\n",
              "      <td>mobydick::sec000::chunk00000</td>\n",
              "      <td>sec_000</td>\n",
              "      <td>*** START OF THE PROJECT GUTENBERG EBOOK MOBY ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>69</td>\n",
              "      <td>*** START OF THE PROJECT GUTENBERG EBOOK MOBY ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mobydick</td>\n",
              "      <td>mobydick::sec001::chunk00000</td>\n",
              "      <td>sec_001</td>\n",
              "      <td>MOBY-DICK;</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>43</td>\n",
              "      <td>MOBY-DICK;\\nor, THE WHALE By Herman Melville</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mobydick</td>\n",
              "      <td>mobydick::sec002::chunk00000</td>\n",
              "      <td>sec_002</td>\n",
              "      <td>CONTENTS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>CONTENTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mobydick</td>\n",
              "      <td>mobydick::sec003::chunk00000</td>\n",
              "      <td>sec_003</td>\n",
              "      <td>ETYMOLOGY.</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>493</td>\n",
              "      <td>1623</td>\n",
              "      <td>ETYMOLOGY EXTRACTS (Supplied by a Sub-Sub-Libr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mobydick</td>\n",
              "      <td>mobydick::sec003::chunk00001</td>\n",
              "      <td>sec_003</td>\n",
              "      <td>ETYMOLOGY.</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>508</td>\n",
              "      <td>1700</td>\n",
              "      <td>CHAPTER 52 The Albatross CHAPTER 53 The Gam CH...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     doc_id                      chunk_id section_id  \\\n",
              "0  mobydick  mobydick::sec000::chunk00000    sec_000   \n",
              "1  mobydick  mobydick::sec001::chunk00000    sec_001   \n",
              "2  mobydick  mobydick::sec002::chunk00000    sec_002   \n",
              "3  mobydick  mobydick::sec003::chunk00000    sec_003   \n",
              "4  mobydick  mobydick::sec003::chunk00001    sec_003   \n",
              "\n",
              "                                       section_title  page_start  page_end  \\\n",
              "0  *** START OF THE PROJECT GUTENBERG EBOOK MOBY ...           1         1   \n",
              "1                                         MOBY-DICK;           1         1   \n",
              "2                                           CONTENTS           1         1   \n",
              "3                                         ETYMOLOGY.           1         6   \n",
              "4                                         ETYMOLOGY.           1         6   \n",
              "\n",
              "   token_count  char_count                                               text  \n",
              "0           20          69  *** START OF THE PROJECT GUTENBERG EBOOK MOBY ...  \n",
              "1           15          43       MOBY-DICK;\\nor, THE WHALE By Herman Melville  \n",
              "2            1           8                                           CONTENTS  \n",
              "3          493        1623  ETYMOLOGY EXTRACTS (Supplied by a Sub-Sub-Libr...  \n",
              "4          508        1700  CHAPTER 52 The Albatross CHAPTER 53 The Gam CH...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Option 1: Load chunks from previously saved parquet file\n",
        "# chunks_df = pd.read_parquet(\"data/processed/pdf_chunks_YYYYMMDD_HHMMSS.parquet\")\n",
        "\n",
        "# Option 2: Generate chunks from PDF (if not already done)\n",
        "pdf_path = \"data/mobydick.pdf\"  # Update this path\n",
        "chunks_df = pdf_to_chunks(\n",
        "    pdf_path=pdf_path,\n",
        "    max_tokens=512,\n",
        "    overlap_tokens=64,\n",
        "    min_tokens=128\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(chunks_df)} chunks\")\n",
        "chunks_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Generate Questions\n",
        "\n",
        "We'll generate multiple questions per chunk using a question generator. The system supports:\n",
        "- **HuggingFace Models**: Uses text-generation models (e.g., DialoGPT) to generate questions\n",
        "- **Heuristic Fallback**: Deterministic question generation if LLM is unavailable\n",
        "\n",
        "### Question Generation Strategy\n",
        "\n",
        "Each chunk will have multiple questions generated, creating diverse query-passage pairs. This diversity helps the model learn robust embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading question generation model: google/gemma-3-27b-it\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# Configure question generation\n",
        "# The system will try HuggingFace first, fall back to heuristic if needed\n",
        "question_gen_config = QuestionGenConfig(\n",
        "    model_name=\"google/gemma-3-27b-it\",  # Lightweight model, can use others\n",
        "    num_questions_per_chunk=3,  # Generate 3 questions per chunk\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Get question generator (automatically falls back to heuristic if HF fails)\n",
        "question_generator = get_question_generator(question_gen_config)\n",
        "\n",
        "# Generate questions for all chunks\n",
        "# This creates (query, passage) pairs\n",
        "pairs_df = generate_questions_dataset(\n",
        "    chunks_df,\n",
        "    question_generator=question_generator,\n",
        "    num_questions_per_chunk=3\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(pairs_df)} query-passage pairs\")\n",
        "print(f\"Average questions per chunk: {len(pairs_df) / len(chunks_df):.2f}\")\n",
        "pairs_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Examine Generated Questions\n",
        "\n",
        "Let's look at some example question-passage pairs to verify quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display example pairs\n",
        "print(\"Example Query-Passage Pairs:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for idx in range(min(5, len(pairs_df))):\n",
        "    pair = pairs_df.iloc[idx]\n",
        "    print(f\"\\nPair {idx + 1}:\")\n",
        "    print(f\"  Query:    '{pair['query']}'\")\n",
        "    print(f\"  Passage:  '{pair['passage'][:150]}...'\")\n",
        "    print(f\"  Section:  {pair['section_title']}\")\n",
        "    print(f\"  Pages:    {pair['page_start']}-{pair['page_end']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Hard Negative Mining (Optional)\n",
        "\n",
        "Hard negative mining finds chunks that are semantically similar but not the correct answer to a query. This improves contrastive learning by providing challenging negative examples.\n",
        "\n",
        "### How Hard Negatives Work\n",
        "\n",
        "1. Embed all chunks (if not already done)\n",
        "2. For each query, find the most similar chunks (excluding the correct passage)\n",
        "3. Add these as negative examples to the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4a: Embed chunks if not already done\n",
        "# (You can skip this if embeddings were generated in notebook 09)\n",
        "from src.models.embedding_pipeline import load_embeddinggemma_model, embed_texts\n",
        "import torch\n",
        "\n",
        "tokenizer, model = load_embeddinggemma_model()\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "# Embed all chunks in batches\n",
        "chunk_texts = chunks_df['text'].tolist()\n",
        "batch_size = 64\n",
        "all_embeddings = []\n",
        "\n",
        "for i in range(0, len(chunk_texts), batch_size):\n",
        "    batch = chunk_texts[i:i+batch_size]\n",
        "    batch_embeddings = embed_texts(batch, model, tokenizer, device=device, max_length=512)\n",
        "    all_embeddings.append(batch_embeddings.numpy())\n",
        "\n",
        "chunk_embeddings = np.vstack(all_embeddings)\n",
        "print(f\"Embedded {len(chunk_embeddings)} chunks\")\n",
        "\n",
        "# Step 4b: Find hard negatives for each query\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Embed queries\n",
        "query_texts = pairs_df['query'].tolist()\n",
        "query_embeddings = []\n",
        "\n",
        "for query in query_texts:\n",
        "    query_emb = embed_texts(query, model, tokenizer, device=device, max_length=512)\n",
        "    query_embeddings.append(query_emb.numpy())\n",
        "\n",
        "query_embeddings = np.vstack(query_embeddings)\n",
        "\n",
        "# For each query, find similar chunks (excluding the correct passage)\n",
        "hard_negatives = []\n",
        "for idx, row in pairs_df.iterrows():\n",
        "    query_emb = query_embeddings[idx:idx+1]\n",
        "    similarities = cosine_similarity(query_emb, chunk_embeddings)[0]\n",
        "    \n",
        "    # Get correct chunk index\n",
        "    correct_chunk_id = row['chunk_id']\n",
        "    correct_chunk_idx = chunks_df[chunks_df['chunk_id'] == correct_chunk_id].index[0]\n",
        "    \n",
        "    # Find top similar chunks (excluding correct one)\n",
        "    top_indices = np.argsort(similarities)[::-1]\n",
        "    top_indices = [i for i in top_indices if i != correct_chunk_idx][:3]  # Top 3 hard negatives\n",
        "    \n",
        "    hard_negatives.append({\n",
        "        'query': row['query'],\n",
        "        'positive': row['passage'],\n",
        "        'hard_negatives': [chunks_df.iloc[i]['text'] for i in top_indices]\n",
        "    })\n",
        "\n",
        "print(f\"Found hard negatives for {len(hard_negatives)} queries\")\n",
        "print(\"\\nExample hard negative:\")\n",
        "print(f\"  Query: {hard_negatives[0]['query']}\")\n",
        "print(f\"  Positive: {hard_negatives[0]['positive'][:100]}...\")\n",
        "print(f\"  Hard negatives: {len(hard_negatives[0]['hard_negatives'])} chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train/Val Split\n",
        "\n",
        "We'll split the data into training and validation sets using **grouped splitting**. This ensures that all pairs from the same chunk go to the same split, preventing data leakage.\n",
        "\n",
        "### Why Grouped Splitting?\n",
        "\n",
        "If we randomly split pairs, pairs from the same chunk might appear in both train and val sets. This leaks information because the model could memorize chunk content from training and see it again in validation. Grouped splitting prevents this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train/val with grouped splitting\n",
        "train_df, val_df = train_val_split_pairs(\n",
        "    pairs_df,\n",
        "    val_ratio=0.2,  # 20% for validation\n",
        "    random_state=42,\n",
        "    group_by='chunk_id'  # Group by chunk to prevent leakage\n",
        ")\n",
        "\n",
        "print(f\"Training pairs: {len(train_df)}\")\n",
        "print(f\"Validation pairs: {len(val_df)}\")\n",
        "print(f\"\\nTrain/Val split statistics:\")\n",
        "print(f\"  Train chunks: {train_df['chunk_id'].nunique()}\")\n",
        "print(f\"  Val chunks: {val_df['chunk_id'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Dataset\n",
        "\n",
        "Save all the generated pairs to timestamped files for use in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all datasets to timestamped files\n",
        "saved_paths = save_questions_dataset(\n",
        "    pairs_df,\n",
        "    train_df=train_df,\n",
        "    val_df=val_df\n",
        ")\n",
        "\n",
        "print(\"Saved files:\")\n",
        "for key, path in saved_paths.items():\n",
        "    print(f\"  {key}: {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. ✅ Question generation from chunks (using LLM or heuristics)\n",
        "2. ✅ Building query-passage pairs for training\n",
        "3. ✅ Hard negative mining for improved contrastive learning\n",
        "4. ✅ Grouped train/val splitting to prevent data leakage\n",
        "5. ✅ Saving datasets to timestamped files\n",
        "\n",
        "**Next Steps:**\n",
        "- Proceed to notebook `11_LoRA_FineTune_EmbeddingGemma_on_PDF_QA.ipynb` to fine-tune the model\n",
        "- The training CSV files are ready to use with the existing training framework\n",
        "- Hard negatives are available in the dataset for future use in advanced training\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
